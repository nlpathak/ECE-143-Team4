{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from models import train_classifier, train_w2v, getExtremeWords, predict, analyzeTweets, getMostSimilarWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Stage + Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/kazanova/sentiment140\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['target','ids','date','flag','user','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "0             0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1             0  is upset that he can't update his Facebook by ...\n",
       "2             0  @Kenichan I dived many times for the ball. Man...\n",
       "3             0    my whole body feels itchy and like its on fire \n",
       "4             0  @nationwideclass no, it's not behaving at all....\n",
       "...         ...                                                ...\n",
       "1599995       4  Just woke up. Having no school is the best fee...\n",
       "1599996       4  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       4  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       4  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_df = df.drop(['ids', 'date', 'flag', 'user'], axis=1) # drop cols that aren't useful for our model\n",
    "final_large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Negative Sentiment Tweets: 800000\n",
      "Number of Neutral Sentiment Tweets: 0\n",
      "Number of Positive Sentiment Tweets: 800000\n"
     ]
    }
   ],
   "source": [
    "print('Number of Negative Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 0]))\n",
    "print('Number of Neutral Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 2]))\n",
    "print('Number of Positive Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice no neutral sentiment data and way too much data!\n",
    "The dataset only holds positive and negative sentiment tweets, so the targets are only 0 or 4. The dataset is also 1.6 million tweets which is too large for our compute resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1594484</th>\n",
       "      <td>1.0</td>\n",
       "      <td>and...my LAST/ BACK ROW CREW!! my SWEAT 6TENer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103711</th>\n",
       "      <td>1.0</td>\n",
       "      <td>heyyy I have 14* new followers last nite &amp;amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530556</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@heycassadee I just like how Cassadee Pope tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007094</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@LilCease Awwww that was sweet of you...thats ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655573</th>\n",
       "      <td>0.0</td>\n",
       "      <td>One of those rainy days in chicago..I wanna st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797663</th>\n",
       "      <td>0.0</td>\n",
       "      <td>has done a lot of work today. The bedroom is v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82245</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I woke up from a dream.  I was playing the slo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826584</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@toddbeltz I'm good! I think your work is like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140017</th>\n",
       "      <td>0.0</td>\n",
       "      <td>fuck! i hate streaming seasons of shows online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153625</th>\n",
       "      <td>1.0</td>\n",
       "      <td>thinks calling people backup plans is positive...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "1594484     1.0  and...my LAST/ BACK ROW CREW!! my SWEAT 6TENer...\n",
       "1103711     1.0  heyyy I have 14* new followers last nite &amp;...\n",
       "1530556     1.0  @heycassadee I just like how Cassadee Pope tre...\n",
       "1007094     1.0  @LilCease Awwww that was sweet of you...thats ...\n",
       "655573      0.0  One of those rainy days in chicago..I wanna st...\n",
       "...         ...                                                ...\n",
       "797663      0.0  has done a lot of work today. The bedroom is v...\n",
       "82245       0.0  I woke up from a dream.  I was playing the slo...\n",
       "826584      1.0  @toddbeltz I'm good! I think your work is like...\n",
       "140017      0.0  fuck! i hate streaming seasons of shows online...\n",
       "1153625     1.0  thinks calling people backup plans is positive...\n",
       "\n",
       "[800000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_df.target = final_large_df.target / 4 # convert the target column to 0 and 1 labels where 1 is positive\n",
    "final_df = final_large_df.sample(800000) # random sample 800k rows\n",
    "del final_large_df, df\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Splits for Model Evaluation\n",
    "Notice with the sampling and splits of the data, we still maintain strong class balance in the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(final_df, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Stats:\n",
      "Size of Training Set: 640000\n",
      "Number of Negative Sentiment Tweets: 320220\n",
      "Number of Positive Sentiment Tweets: 319780\n"
     ]
    }
   ],
   "source": [
    "print('Training Set Stats:')\n",
    "print('Size of Training Set:', len(train_df))\n",
    "print('Number of Negative Sentiment Tweets:', len(train_df[train_df['target'] == 0]))\n",
    "print('Number of Positive Sentiment Tweets:', len(train_df[train_df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Stats:\n",
      "Size of Test Set: 160000\n",
      "Number of Negative Sentiment Tweets: 79864\n",
      "Number of Positive Sentiment Tweets: 80136\n"
     ]
    }
   ],
   "source": [
    "print('Test Set Stats:')\n",
    "print('Size of Test Set:', len(test_df))\n",
    "print('Number of Negative Sentiment Tweets:', len(test_df[test_df['target'] == 0]))\n",
    "print('Number of Positive Sentiment Tweets:', len(test_df[test_df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count-Vectorizer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing data...\n",
      "Training Model...\n",
      "\n",
      "LogisticRegression Classifier using CountVectorizer\n",
      "Training Accuracy: 0.8206421875\n",
      "Testing Accuracy: 0.79059375\n"
     ]
    }
   ],
   "source": [
    "# train function writes the model, vectorizer, and data used to files for use by the web app or other functions\n",
    "train_classifier(train_df, test_df, CountVectorizer, stop_words='english', max_features=200000, ngram_range=(1,3), C=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing data...\n",
      "Training Model...\n",
      "\n",
      "LogisticRegression Classifier using TfidfVectorizer\n",
      "Training Accuracy: 0.84041875\n",
      "Testing Accuracy: 0.79191875\n"
     ]
    }
   ],
   "source": [
    "train_classifier(train_df, test_df, TfidfVectorizer, stop_words='english', max_features=200000, ngram_range=(1,3), C=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate Code for Loading Saved Model/Vectorizer/Data\n",
    "Notice the training and testing accuracies match the ones above, which means we have saved and loaded everything correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled Training Accuracy: 0.84041875\n",
      "Pickled Testing Accuracy: 0.79191875\n"
     ]
    }
   ],
   "source": [
    "# Test how to load the files that were written by train_classifier\n",
    "read_vect = pickle.load(open('./out/tfidf_vect.pickle', 'rb'))\n",
    "read_model = pickle.load(open('./out/tfidf_model.pickle', 'rb'))\n",
    "\n",
    "read_X_train = scipy.sparse.load_npz('./out/tfidf_vect_X_train.npz')\n",
    "read_Y_train = np.load('./out/tfidf_vect_Y_train.npy')\n",
    "read_X_test = scipy.sparse.load_npz('./out/tfidf_vect_X_test.npz')\n",
    "read_Y_test = np.load('./out/tfidf_vect_Y_test.npy')\n",
    "print(f'Pickled Training Accuracy: {np.mean(read_model.predict(read_X_train) == read_Y_train)}')\n",
    "print(f'Pickled Testing Accuracy: {np.mean(read_model.predict(read_X_test) == read_Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Vectorizer Learned Information\n",
    "### Analyze a chosen model and its predictions on some sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['sad', 'sick', 'miss', \"can't\", 'poor', 'missing', 'sadly',\n",
       "        'sucks', 'hurts', 'bummed', 'headache', 'wish', 'unfortunately',\n",
       "        'died', 'gutted', 'ugh', 'hate', 'broke', 'lost', 'disappointed',\n",
       "        'depressing', 'upset', 'missed', 'bummer', 'rip', 'disappointing',\n",
       "        'lonely', 'ruined', 'missin', 'depressed', 'cancelled', 'broken',\n",
       "        'worst', 'sorry', \"didn't\", 'horrible', 'anymore', 'bad', 'sigh',\n",
       "        '. miss', 'closed', 'hurt', 'failed', 'stuck', 'misses', 'damn',\n",
       "        'ouch', 'hates', 'argh', 'hurting'], dtype='<U59'),\n",
       " array([\"can't wait\", 'wish luck', 'thank', 'thanks', 'welcome', 'smile',\n",
       "        'yay', \"don't forget\", '#followfriday', 'glad', 'awesome', '=(',\n",
       "        \"isn't bad\", 'congrats', 'excited', 'proud', 'congratulations',\n",
       "        'happy', 'amazing', 'gotta love', 'blessed', 'smiling', 'loving',\n",
       "        \"don't worry\", 'hehe', 'wonderful', 'let know', 'great', 'hi',\n",
       "        'love', 'sweet', \"don't need\", 'hehehe', 'pleasure', \"wasn't bad\",\n",
       "        '#musicmonday', 'enjoy', 'cheers', 'best', 'hello', 'thx',\n",
       "        \"don't miss\", 'cute', \"isn't ?\", 'fabulous', 'yummy', 'excellent',\n",
       "        'finally', 'makes happy', \"don't sad\"], dtype='<U59'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read_vect = pickle.load(open('./out/count_vect.pickle', 'rb'))\n",
    "# read_model = pickle.load(open('./out/count_model.pickle', 'rb'))\n",
    "read_vect = pickle.load(open('./out/tfidf_vect.pickle', 'rb'))\n",
    "read_model = pickle.load(open('./out/tfidf_model.pickle', 'rb'))\n",
    "\n",
    "getExtremeWords(vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This bag of chips is disgusting yuck', 'Negative', 0.93666026929223),\n",
       " ('i really enjoy riding my bike', 'Positive', 0.7951799929885445),\n",
       " ('This cameraman is nice!', 'Positive', 0.9681563649737266)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This bag of chips is disgusting yuck', 'i really enjoy riding my bike', 'This cameraman is nice!']\n",
    "predict(tweets, vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', 0),\n",
       "  ('bag', 0.11614393968954008),\n",
       "  ('of', 0),\n",
       "  ('chips', -0.012730931477821544),\n",
       "  ('is', 0),\n",
       "  ('disgusting', -3.331450053894557),\n",
       "  ('yuck', -3.587036261276761)],\n",
       " [('i', 0),\n",
       "  ('really', -1.895889860493235),\n",
       "  ('enjoy', 3.9429369045956975),\n",
       "  ('riding', 1.1123229038476234),\n",
       "  ('my', 0),\n",
       "  ('bike', -0.38522295107391946)],\n",
       " [('this', 0),\n",
       "  ('cameraman', 0),\n",
       "  ('is', 0),\n",
       "  ('nice', 3.6284864899735725),\n",
       "  ('!', 2.631494531320365)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzeTweets(tweets, vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model \n",
    "### Generate the most similar words and similarity scores per word in sample tweets for visualization\n",
    "Referenced https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/ and https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this',\n",
       "   [('that', 0.5846254825592041),\n",
       "    ('which', 0.5327487587928772),\n",
       "    ('next', 0.5212511420249939),\n",
       "    ('it', 0.5002129077911377),\n",
       "    ('the', 0.49814873933792114),\n",
       "    ('every', 0.46154963970184326),\n",
       "    ('today', 0.410861611366272),\n",
       "    ('mid', 0.4086470901966095),\n",
       "    ('a', 0.40743839740753174),\n",
       "    ('another', 0.40245521068573)]),\n",
       "  ('bag',\n",
       "   [('box', 0.7764383554458618),\n",
       "    ('shoe', 0.7533471584320068),\n",
       "    ('jacket', 0.74973064661026),\n",
       "    ('bottle', 0.746452808380127),\n",
       "    ('mug', 0.7421192526817322),\n",
       "    ('belt', 0.7348504066467285),\n",
       "    ('pen', 0.7292981147766113),\n",
       "    ('pocket', 0.7237828969955444),\n",
       "    ('pot', 0.7233998775482178),\n",
       "    ('tank', 0.7175831198692322)]),\n",
       "  ('of',\n",
       "   [('obstacle', 0.43935900926589966),\n",
       "    ('recent', 0.434344619512558),\n",
       "    ('actual', 0.4305911362171173),\n",
       "    (\"world's\", 0.4273523688316345),\n",
       "    ('current', 0.41370707750320435),\n",
       "    ('thereof', 0.41030368208885193),\n",
       "    ('parking', 0.4006973206996918),\n",
       "    ('male', 0.3994006812572479),\n",
       "    ('stanley', 0.395061731338501),\n",
       "    ('distinct', 0.3943851888179779)]),\n",
       "  ('chips',\n",
       "   [('strawberries', 0.8989369869232178),\n",
       "    ('cheese', 0.8968567848205566),\n",
       "    ('rice', 0.895643413066864),\n",
       "    ('bacon', 0.8834702968597412),\n",
       "    ('garlic', 0.8831850290298462),\n",
       "    ('burgers', 0.8821228742599487),\n",
       "    ('salad', 0.8792316913604736),\n",
       "    ('potatoes', 0.8736331462860107),\n",
       "    ('eggs', 0.8735617399215698),\n",
       "    ('pork', 0.8732674717903137)]),\n",
       "  ('is',\n",
       "   [(\"isn't\", 0.7178860902786255),\n",
       "    ('isnt', 0.6969406604766846),\n",
       "    ('keeps', 0.6214579939842224),\n",
       "    ('was', 0.6058854460716248),\n",
       "    ('gets', 0.5524137020111084),\n",
       "    (\"she's\", 0.5156582593917847),\n",
       "    ('wasnt', 0.5030219554901123),\n",
       "    ('hes', 0.49947690963745117),\n",
       "    (\"ain't\", 0.4987376928329468),\n",
       "    (\"wasn't\", 0.49685078859329224)]),\n",
       "  ('disgusting',\n",
       "   [('gross', 0.760028064250946),\n",
       "    ('creepy', 0.7266671657562256),\n",
       "    ('painful', 0.7093700170516968),\n",
       "    ('wierd', 0.708908200263977),\n",
       "    ('unbelievable', 0.7054615616798401),\n",
       "    ('ugly', 0.7044612169265747),\n",
       "    ('horrid', 0.696618378162384),\n",
       "    ('uncomfortable', 0.6957048177719116),\n",
       "    ('weird', 0.6854996085166931),\n",
       "    ('terrifying', 0.6752561330795288)]),\n",
       "  ('yuck',\n",
       "   [('bleh', 0.7918062806129456),\n",
       "    ('urgh', 0.7185677289962769),\n",
       "    ('ick', 0.6797367930412292),\n",
       "    ('eww', 0.6653273701667786),\n",
       "    ('ughhh', 0.6552807688713074),\n",
       "    ('ughh', 0.643825352191925),\n",
       "    ('yucky', 0.6274195313453674),\n",
       "    ('blah', 0.6252648830413818),\n",
       "    ('agh', 0.6174744963645935),\n",
       "    ('ugghh', 0.6157369613647461)])],\n",
       " [('i',\n",
       "   [('we', 0.6748764514923096),\n",
       "    ('you', 0.6265665292739868),\n",
       "    ('they', 0.6125273704528809),\n",
       "    ('u', 0.563738226890564),\n",
       "    ('it', 0.559705913066864),\n",
       "    ('them', 0.4914565682411194),\n",
       "    ('ii', 0.4831653833389282),\n",
       "    ('she', 0.4782154858112335),\n",
       "    ('but', 0.4697977900505066),\n",
       "    ('he', 0.4665603041648865)]),\n",
       "  ('really',\n",
       "   [('kinda', 0.6379544734954834),\n",
       "    ('actually', 0.6301078796386719),\n",
       "    ('quite', 0.6214773058891296),\n",
       "    ('so', 0.6006944179534912),\n",
       "    ('too', 0.5520747900009155),\n",
       "    ('seriously', 0.5425688028335571),\n",
       "    ('very', 0.52916419506073),\n",
       "    ('but', 0.5100951790809631),\n",
       "    ('honestly', 0.49972379207611084),\n",
       "    ('that', 0.493354856967926)]),\n",
       "  ('enjoy',\n",
       "   [('enjoying', 0.6312736868858337),\n",
       "    ('enjoyed', 0.5929967761039734),\n",
       "    ('appreciate', 0.4800316393375397),\n",
       "    (\"how's\", 0.4663572907447815),\n",
       "    ('hows', 0.44785791635513306),\n",
       "    ('njoy', 0.43147334456443787),\n",
       "    ('share', 0.41886287927627563),\n",
       "    ('relax', 0.4068836271762848),\n",
       "    ('return', 0.4015862047672272),\n",
       "    ('loving', 0.39646685123443604)]),\n",
       "  ('riding',\n",
       "   [('walking', 0.6831647753715515),\n",
       "    ('ride', 0.6615873575210571),\n",
       "    ('river', 0.6594058275222778),\n",
       "    ('rode', 0.6543664932250977),\n",
       "    ('bridge', 0.6535338759422302),\n",
       "    ('lake', 0.625076413154602),\n",
       "    ('mountain', 0.6190924644470215),\n",
       "    ('trail', 0.6132950186729431),\n",
       "    ('rides', 0.6036361455917358),\n",
       "    ('parked', 0.587023138999939)]),\n",
       "  ('my',\n",
       "   [('his', 0.7686375379562378),\n",
       "    ('her', 0.7242792248725891),\n",
       "    ('your', 0.6525890827178955),\n",
       "    ('mah', 0.5898481607437134),\n",
       "    ('their', 0.5892945528030396),\n",
       "    ('our', 0.5646880865097046),\n",
       "    ('ma', 0.5543116331100464),\n",
       "    ('mine', 0.5226789116859436),\n",
       "    (\"someone's\", 0.49995291233062744),\n",
       "    ('ur', 0.47950422763824463)]),\n",
       "  ('bike',\n",
       "   [('bicycle', 0.7760893106460571),\n",
       "    ('motorcycle', 0.768347978591919),\n",
       "    ('car', 0.7049920558929443),\n",
       "    ('ride', 0.6972765922546387),\n",
       "    ('lawn', 0.680919885635376),\n",
       "    ('truck', 0.6466487646102905),\n",
       "    ('tire', 0.6459747552871704),\n",
       "    ('roof', 0.6030657291412354),\n",
       "    ('horse', 0.6007577180862427),\n",
       "    ('machine', 0.5957369804382324)])],\n",
       " [('this',\n",
       "   [('that', 0.5846254825592041),\n",
       "    ('which', 0.5327487587928772),\n",
       "    ('next', 0.5212511420249939),\n",
       "    ('it', 0.5002129077911377),\n",
       "    ('the', 0.49814873933792114),\n",
       "    ('every', 0.46154963970184326),\n",
       "    ('today', 0.410861611366272),\n",
       "    ('mid', 0.4086470901966095),\n",
       "    ('a', 0.40743839740753174),\n",
       "    ('another', 0.40245521068573)]),\n",
       "  ('cameraman', []),\n",
       "  ('is',\n",
       "   [(\"isn't\", 0.7178860902786255),\n",
       "    ('isnt', 0.6969406604766846),\n",
       "    ('keeps', 0.6214579939842224),\n",
       "    ('was', 0.6058854460716248),\n",
       "    ('gets', 0.5524137020111084),\n",
       "    (\"she's\", 0.5156582593917847),\n",
       "    ('wasnt', 0.5030219554901123),\n",
       "    ('hes', 0.49947690963745117),\n",
       "    (\"ain't\", 0.4987376928329468),\n",
       "    (\"wasn't\", 0.49685078859329224)]),\n",
       "  ('nice',\n",
       "   [('lovely', 0.7600400447845459),\n",
       "    ('great', 0.7351645827293396),\n",
       "    ('cool', 0.7237040996551514),\n",
       "    ('beautiful', 0.6746810674667358),\n",
       "    ('good', 0.6737451553344727),\n",
       "    ('gorgeous', 0.6564163565635681),\n",
       "    ('fab', 0.6432654857635498),\n",
       "    ('fabulous', 0.6344023942947388),\n",
       "    ('fantastic', 0.6272772550582886),\n",
       "    ('wonderful', 0.6123210191726685)]),\n",
       "  ('!',\n",
       "   [('whoop', 0.5386941432952881),\n",
       "    ('hoo', 0.5298551917076111),\n",
       "    ('hooo', 0.49888062477111816),\n",
       "    (':-d', 0.4751439690589905),\n",
       "    ('xxx', 0.4739229083061218),\n",
       "    ('yayyy', 0.4677872657775879),\n",
       "    ('xx', 0.46452048420906067),\n",
       "    ('=d', 0.4569703936576843),\n",
       "    ('omg', 0.45654672384262085),\n",
       "    ('woot', 0.44663554430007935)])]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading model from file\n",
    "read_w2v = Word2Vec.load('./out/word2vec.model')\n",
    "\n",
    "tweets = ['This bag of chips is disgusting yuck', 'i really enjoy riding my bike', 'This cameraman is nice!']\n",
    "getMostSimilarWords(tweets, w2v_model=read_w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
