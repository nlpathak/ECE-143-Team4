{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Stage + Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/kazanova/sentiment140\n",
    "# Added custom column header line to the csv after download\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "df = df.rename(columns={'0': 'target', '1467810369': 'id', 'Mon Apr 06 22:19:45 PDT 2009': 'date', 'NO_QUERY': 'flag', '_TheSpecialOne_': 'user', '@switchfoot http://twitpic.com/2y1zl - Awww, that\\'s a bummer. You shoulda got David Carr of Third Day to do it. ;D': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         target  \\\n",
       "0             0   \n",
       "1             0   \n",
       "2             0   \n",
       "3             0   \n",
       "4             0   \n",
       "...         ...   \n",
       "1599994       4   \n",
       "1599995       4   \n",
       "1599996       4   \n",
       "1599997       4   \n",
       "1599998       4   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@Kwesidei not the whole crew</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599994</th>\n      <td>4</td>\n      <td>Just woke up. Having no school is the best fee...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>4</td>\n      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>4</td>\n      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>4</td>\n      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>4</td>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599999 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "final_large_df = df.drop(['id', 'date', 'flag', 'user'], axis=1) # drop cols that aren't useful for our model\n",
    "final_large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Negative Sentiment Tweets: 799999\nNumber of Neutral Sentiment Tweets: 0\nNumber of Positive Sentiment Tweets: 800000\n"
     ]
    }
   ],
   "source": [
    "print('Number of Negative Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 0]))\n",
    "print('Number of Neutral Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 2]))\n",
    "print('Number of Positive Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice no neutral sentiment data and way too much data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_large_df.target = final_large_df.target / 4 # convert the target column to 0 and 1 labels where 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         target  \\\n",
       "0           0.0   \n",
       "1           0.0   \n",
       "2           0.0   \n",
       "3           0.0   \n",
       "4           0.0   \n",
       "...         ...   \n",
       "1599994     1.0   \n",
       "1599995     1.0   \n",
       "1599996     1.0   \n",
       "1599997     1.0   \n",
       "1599998     1.0   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>@Kwesidei not the whole crew</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599994</th>\n      <td>1.0</td>\n      <td>Just woke up. Having no school is the best fee...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>1.0</td>\n      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>1.0</td>\n      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>1.0</td>\n      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>1.0</td>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599999 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "final_large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_large_df.sample(200000)\n",
    "del final_large_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         target  \\\n",
       "875230      1.0   \n",
       "1252875     1.0   \n",
       "414593      0.0   \n",
       "221361      0.0   \n",
       "150619      0.0   \n",
       "...         ...   \n",
       "1007459     1.0   \n",
       "430633      0.0   \n",
       "1099437     1.0   \n",
       "1080812     1.0   \n",
       "1435647     1.0   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "875230   @HannahxCx  aha, my fav colour is purple whats...                                                                   \n",
       "1252875                                   my phone works!                                                                    \n",
       "414593   @teatotally I want @mollieofficial's 'blonde' ...                                                                   \n",
       "221361   I was in Mosouri last weekend and came back to...                                                                   \n",
       "150619   still hunting for the chicago @nkotb show. its...                                                                   \n",
       "...                                                    ...                                                                   \n",
       "1007459                 @tanyasgoodies Thanks 4 following                                                                    \n",
       "430633                                       @thepjmorton                                                                    \n",
       "1099437  @chorale We have sunshine aplenty here today -...                                                                   \n",
       "1080812  @sadironman RENT was absolutely FANTASTIC. I c...                                                                   \n",
       "1435647                      going to my dads for a visit                                                                    \n",
       "\n",
       "[200000 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>875230</th>\n      <td>1.0</td>\n      <td>@HannahxCx  aha, my fav colour is purple whats...</td>\n    </tr>\n    <tr>\n      <th>1252875</th>\n      <td>1.0</td>\n      <td>my phone works!</td>\n    </tr>\n    <tr>\n      <th>414593</th>\n      <td>0.0</td>\n      <td>@teatotally I want @mollieofficial's 'blonde' ...</td>\n    </tr>\n    <tr>\n      <th>221361</th>\n      <td>0.0</td>\n      <td>I was in Mosouri last weekend and came back to...</td>\n    </tr>\n    <tr>\n      <th>150619</th>\n      <td>0.0</td>\n      <td>still hunting for the chicago @nkotb show. its...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1007459</th>\n      <td>1.0</td>\n      <td>@tanyasgoodies Thanks 4 following</td>\n    </tr>\n    <tr>\n      <th>430633</th>\n      <td>0.0</td>\n      <td>@thepjmorton</td>\n    </tr>\n    <tr>\n      <th>1099437</th>\n      <td>1.0</td>\n      <td>@chorale We have sunshine aplenty here today -...</td>\n    </tr>\n    <tr>\n      <th>1080812</th>\n      <td>1.0</td>\n      <td>@sadironman RENT was absolutely FANTASTIC. I c...</td>\n    </tr>\n    <tr>\n      <th>1435647</th>\n      <td>1.0</td>\n      <td>going to my dads for a visit</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Splits for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(final_df, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Set Stats:\nSize of Training Set: 160000\nNumber of Negative Sentiment Tweets: 80048\nNumber of Positive Sentiment Tweets: 79952\n"
     ]
    }
   ],
   "source": [
    "print('Training Set Stats:')\n",
    "print('Size of Training Set:', len(train_df))\n",
    "print('Number of Negative Sentiment Tweets:', len(train_df[train_df['target'] == 0]))\n",
    "print('Number of Positive Sentiment Tweets:', len(train_df[train_df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Set Stats:\nSize of Test Set: 40000\nNumber of Negative Sentiment Tweets: 20133\nNumber of Positive Sentiment Tweets: 19867\n"
     ]
    }
   ],
   "source": [
    "print('Test Set Stats:')\n",
    "print('Size of Test Set:', len(test_df))\n",
    "print('Number of Negative Sentiment Tweets:', len(test_df[test_df['target'] == 0]))\n",
    "print('Number of Positive Sentiment Tweets:', len(test_df[test_df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count-Vectorizer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f425d856d094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f425d856d094>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,3), max_features=100000)\n",
    "X_train = count_vectorizer.fit_transform([entry['text'] for i, entry in train_df.iterrows()])\n",
    "Y_train = np.array([int(entry['target']) for i, entry in train_df.iterrows()])\n",
    "\n",
    "X_test = count_vectorizer.transform([entry['text'] for i, entry in test_df.iterrows()])\n",
    "Y_test = np.array([int(entry['target']) for i, entry in test_df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=15000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_model = LogisticRegression(C = 0.1, max_iter=15000)\n",
    "count_vect_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.81019375\n",
      "Testing Accuracy: 0.765775\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Accuracy: {np.mean(count_vect_model.predict(X_train) == Y_train)}')\n",
    "print(f'Testing Accuracy: {np.mean(count_vect_model.predict(X_test) == Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write everything to files\n",
    "pickle.dump(count_vectorizer, open('count_vectorizer.pickle', 'wb'))\n",
    "pickle.dump(count_vect_model, open('count_vect_model.pickle', 'wb'))\n",
    "scipy.sparse.save_npz('count_vect_X_train.npz', X_train)\n",
    "np.save('count_vect_Y_train.npy', Y_train)\n",
    "scipy.sparse.save_npz('count_vect_X_test.npz', X_test)\n",
    "np.save('count_vect_Y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled Training Accuracy: 0.81019375\n",
      "Pickled Testing Accuracy: 0.765775\n"
     ]
    }
   ],
   "source": [
    "# Test that pickling is working\n",
    "read_vect = pickle.load(open('count_vectorizer.pickle', 'rb'))\n",
    "read_model = pickle.load(open('count_vect_model.pickle', 'rb'))\n",
    "\n",
    "read_X_train = scipy.sparse.load_npz('count_vect_X_train.npz')\n",
    "read_Y_train = np.load('count_vect_Y_train.npy')\n",
    "read_X_test = scipy.sparse.load_npz('count_vect_X_test.npz')\n",
    "read_Y_test = np.load('count_vect_Y_test.npy')\n",
    "print(f'Pickled Training Accuracy: {np.mean(read_model.predict(read_X_train) == read_Y_train)}')\n",
    "print(f'Pickled Testing Accuracy: {np.mean(read_model.predict(read_X_test) == read_Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81019375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check probabilities work as expected\n",
    "np.mean(np.argmax(read_model.predict_proba(read_X_train), axis=1) == read_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_features=100000)\n",
    "X_train = tfidf_vectorizer.fit_transform([entry['text'] for i, entry in train_df.iterrows()])\n",
    "Y_train = np.array([int(entry['target']) for i, entry in train_df.iterrows()])\n",
    "\n",
    "X_test = tfidf_vectorizer.transform([entry['text'] for i, entry in test_df.iterrows()])\n",
    "Y_test = np.array([int(entry['target']) for i, entry in test_df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.5, max_iter=15000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model = LogisticRegression(C = 1.5, max_iter=15000)\n",
    "tfidf_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.84886875\n",
      "Testing Accuracy: 0.770725\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Accuracy: {np.mean(tfidf_model.predict(X_train) == Y_train)}')\n",
    "print(f'Testing Accuracy: {np.mean(tfidf_model.predict(X_test) == Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vectorizer, open('tfidf_vect.pickle', 'wb'))\n",
    "pickle.dump(tfidf_model, open('tfidf_model.pickle', 'wb'))\n",
    "scipy.sparse.save_npz('tfidf_vect_X_train.npz', X_train)\n",
    "np.save('tfidf_vect_Y_train.npy', Y_train)\n",
    "scipy.sparse.save_npz('tfidf_vect_X_test.npz', X_test)\n",
    "np.save('tfidf_vect_Y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled Training Accuracy: 0.84886875\n",
      "Pickled Testing Accuracy: 0.770725\n"
     ]
    }
   ],
   "source": [
    "# Test that pickling is working\n",
    "read_vect = pickle.load(open('tfidf_vect.pickle', 'rb'))\n",
    "read_model = pickle.load(open('tfidf_model.pickle', 'rb'))\n",
    "\n",
    "read_X_train = scipy.sparse.load_npz('tfidf_vect_X_train.npz')\n",
    "read_Y_train = np.load('tfidf_vect_Y_train.npy')\n",
    "read_X_test = scipy.sparse.load_npz('tfidf_vect_X_test.npz')\n",
    "read_Y_test = np.load('tfidf_vect_Y_test.npy')\n",
    "print(f'Pickled Training Accuracy: {np.mean(read_model.predict(read_X_train) == read_Y_train)}')\n",
    "print(f'Pickled Testing Accuracy: {np.mean(read_model.predict(read_X_test) == read_Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Vectorizer Learned Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExtremeWords(vectorizer, model):\n",
    "    feature_names = np.array(read_vect.get_feature_names())\n",
    "    order = np.argsort(read_model.coef_)\n",
    "    \n",
    "    print(\"Top 50 Most Negative Words/Phrases in Order:\")\n",
    "    print(feature_names[order[0, :50]])\n",
    "    print()\n",
    "    print(\"Top 50 Most Positive Words/Phrases in Order:\")\n",
    "    print(feature_names[order[0, -50:]][::-1])\n",
    "    \n",
    "    return feature_names[order[0, :50]], feature_names[order[0, -50:]][::-1] # negative, positive    \n",
    "\n",
    "def predict(tweets, vectorizer, model):\n",
    "    tweet_vectors = vectorizer.transform(tweets)\n",
    "    preds = model.predict_proba(tweet_vectors)\n",
    "    returnList = []\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        print(f'Tweet: {tweet}')\n",
    "        pred = \"Negative\" if np.argmax(preds[i]) == 0 else \"Positive\"\n",
    "        print(f'Prediction: {pred}')\n",
    "        print(f'Confidence of {pred} Prediction (0 to 1): {np.max(preds[i])}')\n",
    "        print()\n",
    "        returnList.append((tweet, pred, np.max(preds[i])))\n",
    "    return returnList\n",
    "\n",
    "def analyzeTweets(tweets, vectorizer, model):\n",
    "    returnList = []\n",
    "    for tweet in tweets:\n",
    "        tweetList = []\n",
    "        for word in tweet.split():\n",
    "            word = word.lower()\n",
    "            if word in vectorizer.get_feature_names():\n",
    "                index = vectorizer.get_feature_names().index(word)\n",
    "                print(f'Word: {word}, Connotation: {model.coef_[0, index]:.3f}')\n",
    "                tweetList.append((word, model.coef_[0, index]))\n",
    "            else: # not a top feature\n",
    "                print(f'Word: {word}, Connotation: {0:.3f}')\n",
    "                tweetList.append((word, 0))\n",
    "        returnList.append(tweetList)\n",
    "        print()\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Most Negative Words/Phrases in Order:\n",
      "['sad' 'sick' 'miss' 'poor' 'unfortunately' 'wish' 'sucks' 'hurts'\n",
      " 'missing' 'hate' 'ugh' 'sadly' 'headache' 'bummed' 'died' 'sorry'\n",
      " 'disappointed' 'broke' 'bad' 'bummer' 'lost' 'upset' 'didn' 'shame'\n",
      " 'lonely' 'cancelled' 'missed' 'gutted' 'damn' 'broken' 'rip' 'closed'\n",
      " 'crying' 'anymore' 'horrible' 'worst' 'sore' 'hurt' 'didnt' 'stuck'\n",
      " 'depressing' 'misses' 'stupid' 'depressed' 'booo' 'doesn' 'awful' 'hates'\n",
      " 'killing' 'burnt']\n",
      "\n",
      "Top 50 Most Positive Words/Phrases in Order:\n",
      "['thank' 'thanks' 'welcome' 'congrats' 'smile' 'excited' 'yay' 'wish luck'\n",
      " 'awesome' 'glad' 'great' 'love' 'congratulations' 'wasn bad' 'hi'\n",
      " 'smiling' 'cute' 'amazing' 'hehe' 'happy' 'wonderful' 'haha' 'enjoy'\n",
      " 'proud' 'hello' 'loving' 'pleasure' 'nice' 'adorable' 'hey' 'cool'\n",
      " 'beautiful' 'woo' 'www' 'yummy' 'hehehe' 'fantastic' 'followfriday'\n",
      " 'lovely' 'thx' 'good' 'perfect' 'heh' 'excellent' 'hard work' 'listening'\n",
      " 'sweet' 'woohoo' 'best' 'don need']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['sad', 'sick', 'miss', 'poor', 'unfortunately', 'wish', 'sucks',\n",
       "        'hurts', 'missing', 'hate', 'ugh', 'sadly', 'headache', 'bummed',\n",
       "        'died', 'sorry', 'disappointed', 'broke', 'bad', 'bummer', 'lost',\n",
       "        'upset', 'didn', 'shame', 'lonely', 'cancelled', 'missed',\n",
       "        'gutted', 'damn', 'broken', 'rip', 'closed', 'crying', 'anymore',\n",
       "        'horrible', 'worst', 'sore', 'hurt', 'didnt', 'stuck',\n",
       "        'depressing', 'misses', 'stupid', 'depressed', 'booo', 'doesn',\n",
       "        'awful', 'hates', 'killing', 'burnt'], dtype='<U59'),\n",
       " array(['thank', 'thanks', 'welcome', 'congrats', 'smile', 'excited',\n",
       "        'yay', 'wish luck', 'awesome', 'glad', 'great', 'love',\n",
       "        'congratulations', 'wasn bad', 'hi', 'smiling', 'cute', 'amazing',\n",
       "        'hehe', 'happy', 'wonderful', 'haha', 'enjoy', 'proud', 'hello',\n",
       "        'loving', 'pleasure', 'nice', 'adorable', 'hey', 'cool',\n",
       "        'beautiful', 'woo', 'www', 'yummy', 'hehehe', 'fantastic',\n",
       "        'followfriday', 'lovely', 'thx', 'good', 'perfect', 'heh',\n",
       "        'excellent', 'hard work', 'listening', 'sweet', 'woohoo', 'best',\n",
       "        'don need'], dtype='<U59'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read_vect = pickle.load(open('count_vectorizer.pickle', 'rb'))\n",
    "# read_model = pickle.load(open('count_vect_model.pickle', 'rb'))\n",
    "read_vect = pickle.load(open('tfidf_vect.pickle', 'rb'))\n",
    "read_model = pickle.load(open('tfidf_model.pickle', 'rb'))\n",
    "\n",
    "getExtremeWords(vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: This bag of chips is disgusting yuck\n",
      "Prediction: Negative\n",
      "Confidence of Negative Prediction (0 to 1): 0.769240077207706\n",
      "\n",
      "Tweet: i really enjoy riding my bike\n",
      "Prediction: Positive\n",
      "Confidence of Positive Prediction (0 to 1): 0.6867906070710776\n",
      "\n",
      "Tweet: it will be 70 degrees tomorrow\n",
      "Prediction: Negative\n",
      "Confidence of Negative Prediction (0 to 1): 0.6523543402046126\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('This bag of chips is disgusting yuck', 'Negative', 0.769240077207706),\n",
       " ('i really enjoy riding my bike', 'Positive', 0.6867906070710776),\n",
       " ('it will be 70 degrees tomorrow', 'Negative', 0.6523543402046126)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This bag of chips is disgusting yuck', 'i really enjoy riding my bike', 'it will be 70 degrees tomorrow']\n",
    "predict(tweets, vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: this, Connotation: 0.000\n",
      "Word: bag, Connotation: 0.287\n",
      "Word: of, Connotation: 0.000\n",
      "Word: chips, Connotation: 0.053\n",
      "Word: is, Connotation: 0.000\n",
      "Word: disgusting, Connotation: -1.885\n",
      "Word: yuck, Connotation: -1.171\n",
      "\n",
      "Word: i, Connotation: 0.000\n",
      "Word: really, Connotation: -1.614\n",
      "Word: enjoy, Connotation: 3.597\n",
      "Word: riding, Connotation: -0.327\n",
      "Word: my, Connotation: 0.000\n",
      "Word: bike, Connotation: -0.266\n",
      "\n",
      "Word: it, Connotation: 0.000\n",
      "Word: will, Connotation: 0.000\n",
      "Word: be, Connotation: 0.000\n",
      "Word: 70, Connotation: 0.026\n",
      "Word: degrees, Connotation: -1.080\n",
      "Word: tomorrow, Connotation: -0.761\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('this', 0),\n",
       "  ('bag', 0.2874278446361069),\n",
       "  ('of', 0),\n",
       "  ('chips', 0.05292501433667785),\n",
       "  ('is', 0),\n",
       "  ('disgusting', -1.8845646237164995),\n",
       "  ('yuck', -1.1713829204188018)],\n",
       " [('i', 0),\n",
       "  ('really', -1.6136558188748473),\n",
       "  ('enjoy', 3.5967345548108685),\n",
       "  ('riding', -0.32724736526303483),\n",
       "  ('my', 0),\n",
       "  ('bike', -0.2662721397711939)],\n",
       " [('it', 0),\n",
       "  ('will', 0),\n",
       "  ('be', 0),\n",
       "  ('70', 0.02597830123092517),\n",
       "  ('degrees', -1.0801505686959454),\n",
       "  ('tomorrow', -0.7607353447835417)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzeTweets(tweets, vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model \n",
    "### Generate similarity scores of words for visualization\n",
    "Referenced https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/ and https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [entry['text'].lower() for i, entry in train_df.iterrows()]\n",
    "words = [nltk.word_tokenize(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(words, min_count=5)\n",
    "w2v.save('word2vec.model') # write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostSimilarWords(tweets, w2v_model):\n",
    "    returnList = []\n",
    "    for tweet in tweets:\n",
    "        tweetList = []\n",
    "        for word in nltk.word_tokenize(tweet.lower()):\n",
    "            if word in w2v_model.wv.vocab:\n",
    "                tweetList.append((word, w2v_model.wv.most_similar(word)))\n",
    "            else:\n",
    "                tweetList.append((word, []))\n",
    "        returnList.append(tweetList)\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this',\n",
       "   [('next', 0.5328178405761719),\n",
       "    ('which', 0.5173956155776978),\n",
       "    ('that', 0.5160244107246399),\n",
       "    ('every', 0.4625844657421112),\n",
       "    ('another', 0.45456063747406006),\n",
       "    ('the', 0.432054340839386),\n",
       "    ('whole', 0.4233904182910919),\n",
       "    ('it', 0.4096474051475525),\n",
       "    ('college', 0.39660054445266724),\n",
       "    ('forecast', 0.380306214094162)]),\n",
       "  ('bag',\n",
       "   [('box', 0.8275642395019531),\n",
       "    ('truck', 0.8130146861076355),\n",
       "    ('glass', 0.7917367219924927),\n",
       "    ('bottle', 0.7726737856864929),\n",
       "    ('pot', 0.7644366025924683),\n",
       "    ('bowl', 0.7644003629684448),\n",
       "    ('lump', 0.7609724998474121),\n",
       "    ('pair', 0.7606635689735413),\n",
       "    ('woods', 0.7596918940544128),\n",
       "    ('wall', 0.7590094804763794)]),\n",
       "  ('of',\n",
       "   [('parking', 0.4510842561721802),\n",
       "    ('other', 0.4028436839580536),\n",
       "    ('words', 0.4018063545227051),\n",
       "    ('whole', 0.3989813029766083),\n",
       "    ('row', 0.39098289608955383),\n",
       "    ('social', 0.390688419342041),\n",
       "    ('single', 0.3854885697364807),\n",
       "    ('entire', 0.38478243350982666),\n",
       "    ('moisture', 0.37098002433776855),\n",
       "    ('tree', 0.3705523908138275)]),\n",
       "  ('chips',\n",
       "   [('salad', 0.9031486511230469),\n",
       "    ('cheese', 0.8956888318061829),\n",
       "    ('rice', 0.8870056867599487),\n",
       "    ('eggs', 0.8684196472167969),\n",
       "    ('soup', 0.8678721189498901),\n",
       "    ('yogurt', 0.8581173419952393),\n",
       "    ('sandwich', 0.8567512035369873),\n",
       "    ('juice', 0.8536888957023621),\n",
       "    ('pasta', 0.8535647392272949),\n",
       "    ('cupcakes', 0.8534532785415649)]),\n",
       "  ('is',\n",
       "   [('isnt', 0.7100571393966675),\n",
       "    (\"'s\", 0.6789045333862305),\n",
       "    ('gets', 0.6209675073623657),\n",
       "    ('keeps', 0.5636969804763794),\n",
       "    ('was', 0.5560624599456787),\n",
       "    ('goes', 0.5374260544776917),\n",
       "    ('works', 0.5345250368118286),\n",
       "    ('hes', 0.5171153545379639),\n",
       "    ('stays', 0.5156515836715698),\n",
       "    ('makes', 0.4990745782852173)]),\n",
       "  ('disgusting',\n",
       "   [('gross', 0.8299711346626282),\n",
       "    ('harsh', 0.8140682578086853),\n",
       "    ('concerned', 0.8115346431732178),\n",
       "    ('irritated', 0.7917789220809937),\n",
       "    ('creepy', 0.7906482219696045),\n",
       "    ('icky', 0.7895397543907166),\n",
       "    ('wierd', 0.7874885201454163),\n",
       "    ('aggravated', 0.7862415313720703),\n",
       "    ('amusing', 0.7858133316040039),\n",
       "    ('dope', 0.7809571623802185)]),\n",
       "  ('yuck',\n",
       "   [('eww', 0.8145696520805359),\n",
       "    ('eugh', 0.8021174669265747),\n",
       "    ('uggh', 0.7962573766708374),\n",
       "    ('boredd', 0.7900334596633911),\n",
       "    ('seeds', 0.7831972241401672),\n",
       "    ('cramp', 0.7825180888175964),\n",
       "    ('103', 0.7789351940155029),\n",
       "    ('throbbing', 0.7741044759750366),\n",
       "    ('fractured', 0.7738236784934998),\n",
       "    ('urgh', 0.773102879524231)])],\n",
       " [('i',\n",
       "   [('you', 0.5384178757667542),\n",
       "    ('we', 0.4923892617225647),\n",
       "    ('them', 0.44683951139450073),\n",
       "    ('someone', 0.44173020124435425),\n",
       "    ('they', 0.43754279613494873),\n",
       "    ('u', 0.43227866291999817),\n",
       "    ('never', 0.4290403127670288),\n",
       "    ('myself', 0.42721027135849),\n",
       "    ('actually', 0.4260353147983551),\n",
       "    ('it', 0.42138466238975525)]),\n",
       "  ('really',\n",
       "   [('actually', 0.5876472592353821),\n",
       "    ('so', 0.5788001418113708),\n",
       "    ('kinda', 0.5476610660552979),\n",
       "    ('seriously', 0.5272969603538513),\n",
       "    ('quite', 0.5146756172180176),\n",
       "    ('but', 0.503903865814209),\n",
       "    ('very', 0.4985109567642212),\n",
       "    ('cause', 0.4963245987892151),\n",
       "    ('too', 0.49471187591552734),\n",
       "    ('totally', 0.49431902170181274)]),\n",
       "  ('enjoy',\n",
       "   [('hows', 0.5238146185874939),\n",
       "    ('enjoying', 0.5203596353530884),\n",
       "    ('appreciate', 0.516171932220459),\n",
       "    ('brighten', 0.5088894367218018),\n",
       "    ('enjoyed', 0.4969700574874878),\n",
       "    ('spend', 0.48002418875694275),\n",
       "    ('ruin', 0.47986429929733276),\n",
       "    ('bring', 0.4741554260253906),\n",
       "    ('brightened', 0.4719090461730957),\n",
       "    ('consider', 0.47071292996406555)]),\n",
       "  ('riding',\n",
       "   [('polo', 0.8213859796524048),\n",
       "    ('summertime', 0.7979703545570374),\n",
       "    ('austin', 0.7779415845870972),\n",
       "    ('safeway', 0.7734822630882263),\n",
       "    ('bonfire', 0.7638850212097168),\n",
       "    ('cabin', 0.7627143263816833),\n",
       "    ('stadium', 0.7606664896011353),\n",
       "    ('shooting', 0.7568446397781372),\n",
       "    ('salt', 0.7559365034103394),\n",
       "    ('security', 0.7533419728279114)]),\n",
       "  ('my',\n",
       "   [('his', 0.773335337638855),\n",
       "    ('her', 0.7233325242996216),\n",
       "    ('your', 0.6160921454429626),\n",
       "    ('their', 0.5710585117340088),\n",
       "    ('broken', 0.5329567790031433),\n",
       "    ('our', 0.47636982798576355),\n",
       "    ('mine', 0.4571440517902374),\n",
       "    ('wisdom', 0.4548577070236206),\n",
       "    ('cell', 0.44526413083076477),\n",
       "    ('mah', 0.43581822514533997)]),\n",
       "  ('bike',\n",
       "   [('flat', 0.7586744427680969),\n",
       "    ('seat', 0.7234489321708679),\n",
       "    ('car', 0.72301185131073),\n",
       "    ('truck', 0.7213912606239319),\n",
       "    ('machine', 0.719843864440918),\n",
       "    ('training', 0.7046586275100708),\n",
       "    ('studio', 0.7018383741378784),\n",
       "    ('wall', 0.6994326114654541),\n",
       "    ('ride', 0.69162517786026),\n",
       "    ('purse', 0.690599799156189)])],\n",
       " [('it',\n",
       "   [('that', 0.730560839176178),\n",
       "    ('everything', 0.628578782081604),\n",
       "    ('mine', 0.622330904006958),\n",
       "    ('he', 0.6018409132957458),\n",
       "    ('she', 0.5854959487915039),\n",
       "    ('there', 0.537086009979248),\n",
       "    ('them', 0.5296961069107056),\n",
       "    ('which', 0.5108294486999512),\n",
       "    ('anything', 0.4904855787754059),\n",
       "    ('someone', 0.46590882539749146)]),\n",
       "  ('will',\n",
       "   [(\"'ll\", 0.9148485064506531),\n",
       "    ('may', 0.8334226012229919),\n",
       "    ('might', 0.8265534043312073),\n",
       "    ('shall', 0.7850577235221863),\n",
       "    ('should', 0.7581053972244263),\n",
       "    ('can', 0.7288609743118286),\n",
       "    ('ill', 0.7114196419715881),\n",
       "    ('must', 0.7029680609703064),\n",
       "    ('wont', 0.6812708377838135),\n",
       "    (\"'d\", 0.6352842450141907)]),\n",
       "  ('be',\n",
       "   [('stay', 0.7258485555648804),\n",
       "    ('happen', 0.634366512298584),\n",
       "    ('b', 0.611335813999176),\n",
       "    ('pass', 0.6051467657089233),\n",
       "    ('start', 0.6016355752944946),\n",
       "    ('return', 0.5985159873962402),\n",
       "    ('continue', 0.5948166847229004),\n",
       "    ('fall', 0.583820104598999),\n",
       "    ('die', 0.5805956125259399),\n",
       "    ('stick', 0.5803499221801758)]),\n",
       "  ('70',\n",
       "   [('80', 0.8357950448989868),\n",
       "    ('93', 0.8183032274246216),\n",
       "    ('81', 0.8132692575454712),\n",
       "    ('250', 0.8082927465438843),\n",
       "    ('martin', 0.8042478561401367),\n",
       "    ('minimum', 0.8037912845611572),\n",
       "    ('guild', 0.8034136295318604),\n",
       "    ('58', 0.8021272420883179),\n",
       "    ('tabs', 0.8006020784378052),\n",
       "    ('90', 0.796045184135437)]),\n",
       "  ('degrees',\n",
       "   [('90', 0.7646181583404541),\n",
       "    ('pouring', 0.7449544668197632),\n",
       "    ('cloudy', 0.7187682390213013),\n",
       "    ('windy', 0.7047654390335083),\n",
       "    ('counting', 0.6749802827835083),\n",
       "    ('23', 0.6743985414505005),\n",
       "    ('rainin', 0.6647361516952515),\n",
       "    ('27', 0.6644787192344666),\n",
       "    ('rained', 0.6633415222167969),\n",
       "    ('gray', 0.6605542302131653)]),\n",
       "  ('tomorrow',\n",
       "   [('monday', 0.6343111395835876),\n",
       "    ('tomorow', 0.6145380735397339),\n",
       "    ('tmrw', 0.6117316484451294),\n",
       "    ('later', 0.5961143970489502),\n",
       "    ('2morrow', 0.58842933177948),\n",
       "    ('wednesday', 0.5760573744773865),\n",
       "    ('study', 0.5754880309104919),\n",
       "    ('thursday', 0.5693942308425903),\n",
       "    ('packing', 0.5689973831176758),\n",
       "    ('maths', 0.563879132270813)])]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This bag of chips is disgusting yuck', 'i really enjoy riding my bike', 'it will be 70 degrees tomorrow']\n",
    "getMostSimilarWords(tweets, w2v_model=w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this',\n",
       "   [('next', 0.5328178405761719),\n",
       "    ('which', 0.5173956155776978),\n",
       "    ('that', 0.5160244107246399),\n",
       "    ('every', 0.4625844657421112),\n",
       "    ('another', 0.45456063747406006),\n",
       "    ('the', 0.432054340839386),\n",
       "    ('whole', 0.4233904182910919),\n",
       "    ('it', 0.4096474051475525),\n",
       "    ('college', 0.39660054445266724),\n",
       "    ('forecast', 0.380306214094162)]),\n",
       "  ('bag',\n",
       "   [('box', 0.8275642395019531),\n",
       "    ('truck', 0.8130146861076355),\n",
       "    ('glass', 0.7917367219924927),\n",
       "    ('bottle', 0.7726737856864929),\n",
       "    ('pot', 0.7644366025924683),\n",
       "    ('bowl', 0.7644003629684448),\n",
       "    ('lump', 0.7609724998474121),\n",
       "    ('pair', 0.7606635689735413),\n",
       "    ('woods', 0.7596918940544128),\n",
       "    ('wall', 0.7590094804763794)]),\n",
       "  ('of',\n",
       "   [('parking', 0.4510842561721802),\n",
       "    ('other', 0.4028436839580536),\n",
       "    ('words', 0.4018063545227051),\n",
       "    ('whole', 0.3989813029766083),\n",
       "    ('row', 0.39098289608955383),\n",
       "    ('social', 0.390688419342041),\n",
       "    ('single', 0.3854885697364807),\n",
       "    ('entire', 0.38478243350982666),\n",
       "    ('moisture', 0.37098002433776855),\n",
       "    ('tree', 0.3705523908138275)]),\n",
       "  ('chips',\n",
       "   [('salad', 0.9031486511230469),\n",
       "    ('cheese', 0.8956888318061829),\n",
       "    ('rice', 0.8870056867599487),\n",
       "    ('eggs', 0.8684196472167969),\n",
       "    ('soup', 0.8678721189498901),\n",
       "    ('yogurt', 0.8581173419952393),\n",
       "    ('sandwich', 0.8567512035369873),\n",
       "    ('juice', 0.8536888957023621),\n",
       "    ('pasta', 0.8535647392272949),\n",
       "    ('cupcakes', 0.8534532785415649)]),\n",
       "  ('is',\n",
       "   [('isnt', 0.7100571393966675),\n",
       "    (\"'s\", 0.6789045333862305),\n",
       "    ('gets', 0.6209675073623657),\n",
       "    ('keeps', 0.5636969804763794),\n",
       "    ('was', 0.5560624599456787),\n",
       "    ('goes', 0.5374260544776917),\n",
       "    ('works', 0.5345250368118286),\n",
       "    ('hes', 0.5171153545379639),\n",
       "    ('stays', 0.5156515836715698),\n",
       "    ('makes', 0.4990745782852173)]),\n",
       "  ('disgusting',\n",
       "   [('gross', 0.8299711346626282),\n",
       "    ('harsh', 0.8140682578086853),\n",
       "    ('concerned', 0.8115346431732178),\n",
       "    ('irritated', 0.7917789220809937),\n",
       "    ('creepy', 0.7906482219696045),\n",
       "    ('icky', 0.7895397543907166),\n",
       "    ('wierd', 0.7874885201454163),\n",
       "    ('aggravated', 0.7862415313720703),\n",
       "    ('amusing', 0.7858133316040039),\n",
       "    ('dope', 0.7809571623802185)]),\n",
       "  ('yuck',\n",
       "   [('eww', 0.8145696520805359),\n",
       "    ('eugh', 0.8021174669265747),\n",
       "    ('uggh', 0.7962573766708374),\n",
       "    ('boredd', 0.7900334596633911),\n",
       "    ('seeds', 0.7831972241401672),\n",
       "    ('cramp', 0.7825180888175964),\n",
       "    ('103', 0.7789351940155029),\n",
       "    ('throbbing', 0.7741044759750366),\n",
       "    ('fractured', 0.7738236784934998),\n",
       "    ('urgh', 0.773102879524231)])],\n",
       " [('i',\n",
       "   [('you', 0.5384178757667542),\n",
       "    ('we', 0.4923892617225647),\n",
       "    ('them', 0.44683951139450073),\n",
       "    ('someone', 0.44173020124435425),\n",
       "    ('they', 0.43754279613494873),\n",
       "    ('u', 0.43227866291999817),\n",
       "    ('never', 0.4290403127670288),\n",
       "    ('myself', 0.42721027135849),\n",
       "    ('actually', 0.4260353147983551),\n",
       "    ('it', 0.42138466238975525)]),\n",
       "  ('really',\n",
       "   [('actually', 0.5876472592353821),\n",
       "    ('so', 0.5788001418113708),\n",
       "    ('kinda', 0.5476610660552979),\n",
       "    ('seriously', 0.5272969603538513),\n",
       "    ('quite', 0.5146756172180176),\n",
       "    ('but', 0.503903865814209),\n",
       "    ('very', 0.4985109567642212),\n",
       "    ('cause', 0.4963245987892151),\n",
       "    ('too', 0.49471187591552734),\n",
       "    ('totally', 0.49431902170181274)]),\n",
       "  ('enjoy',\n",
       "   [('hows', 0.5238146185874939),\n",
       "    ('enjoying', 0.5203596353530884),\n",
       "    ('appreciate', 0.516171932220459),\n",
       "    ('brighten', 0.5088894367218018),\n",
       "    ('enjoyed', 0.4969700574874878),\n",
       "    ('spend', 0.48002418875694275),\n",
       "    ('ruin', 0.47986429929733276),\n",
       "    ('bring', 0.4741554260253906),\n",
       "    ('brightened', 0.4719090461730957),\n",
       "    ('consider', 0.47071292996406555)]),\n",
       "  ('riding',\n",
       "   [('polo', 0.8213859796524048),\n",
       "    ('summertime', 0.7979703545570374),\n",
       "    ('austin', 0.7779415845870972),\n",
       "    ('safeway', 0.7734822630882263),\n",
       "    ('bonfire', 0.7638850212097168),\n",
       "    ('cabin', 0.7627143263816833),\n",
       "    ('stadium', 0.7606664896011353),\n",
       "    ('shooting', 0.7568446397781372),\n",
       "    ('salt', 0.7559365034103394),\n",
       "    ('security', 0.7533419728279114)]),\n",
       "  ('my',\n",
       "   [('his', 0.773335337638855),\n",
       "    ('her', 0.7233325242996216),\n",
       "    ('your', 0.6160921454429626),\n",
       "    ('their', 0.5710585117340088),\n",
       "    ('broken', 0.5329567790031433),\n",
       "    ('our', 0.47636982798576355),\n",
       "    ('mine', 0.4571440517902374),\n",
       "    ('wisdom', 0.4548577070236206),\n",
       "    ('cell', 0.44526413083076477),\n",
       "    ('mah', 0.43581822514533997)]),\n",
       "  ('bike',\n",
       "   [('flat', 0.7586744427680969),\n",
       "    ('seat', 0.7234489321708679),\n",
       "    ('car', 0.72301185131073),\n",
       "    ('truck', 0.7213912606239319),\n",
       "    ('machine', 0.719843864440918),\n",
       "    ('training', 0.7046586275100708),\n",
       "    ('studio', 0.7018383741378784),\n",
       "    ('wall', 0.6994326114654541),\n",
       "    ('ride', 0.69162517786026),\n",
       "    ('purse', 0.690599799156189)])],\n",
       " [('it',\n",
       "   [('that', 0.730560839176178),\n",
       "    ('everything', 0.628578782081604),\n",
       "    ('mine', 0.622330904006958),\n",
       "    ('he', 0.6018409132957458),\n",
       "    ('she', 0.5854959487915039),\n",
       "    ('there', 0.537086009979248),\n",
       "    ('them', 0.5296961069107056),\n",
       "    ('which', 0.5108294486999512),\n",
       "    ('anything', 0.4904855787754059),\n",
       "    ('someone', 0.46590882539749146)]),\n",
       "  ('will',\n",
       "   [(\"'ll\", 0.9148485064506531),\n",
       "    ('may', 0.8334226012229919),\n",
       "    ('might', 0.8265534043312073),\n",
       "    ('shall', 0.7850577235221863),\n",
       "    ('should', 0.7581053972244263),\n",
       "    ('can', 0.7288609743118286),\n",
       "    ('ill', 0.7114196419715881),\n",
       "    ('must', 0.7029680609703064),\n",
       "    ('wont', 0.6812708377838135),\n",
       "    (\"'d\", 0.6352842450141907)]),\n",
       "  ('be',\n",
       "   [('stay', 0.7258485555648804),\n",
       "    ('happen', 0.634366512298584),\n",
       "    ('b', 0.611335813999176),\n",
       "    ('pass', 0.6051467657089233),\n",
       "    ('start', 0.6016355752944946),\n",
       "    ('return', 0.5985159873962402),\n",
       "    ('continue', 0.5948166847229004),\n",
       "    ('fall', 0.583820104598999),\n",
       "    ('die', 0.5805956125259399),\n",
       "    ('stick', 0.5803499221801758)]),\n",
       "  ('70',\n",
       "   [('80', 0.8357950448989868),\n",
       "    ('93', 0.8183032274246216),\n",
       "    ('81', 0.8132692575454712),\n",
       "    ('250', 0.8082927465438843),\n",
       "    ('martin', 0.8042478561401367),\n",
       "    ('minimum', 0.8037912845611572),\n",
       "    ('guild', 0.8034136295318604),\n",
       "    ('58', 0.8021272420883179),\n",
       "    ('tabs', 0.8006020784378052),\n",
       "    ('90', 0.796045184135437)]),\n",
       "  ('degrees',\n",
       "   [('90', 0.7646181583404541),\n",
       "    ('pouring', 0.7449544668197632),\n",
       "    ('cloudy', 0.7187682390213013),\n",
       "    ('windy', 0.7047654390335083),\n",
       "    ('counting', 0.6749802827835083),\n",
       "    ('23', 0.6743985414505005),\n",
       "    ('rainin', 0.6647361516952515),\n",
       "    ('27', 0.6644787192344666),\n",
       "    ('rained', 0.6633415222167969),\n",
       "    ('gray', 0.6605542302131653)]),\n",
       "  ('tomorrow',\n",
       "   [('monday', 0.6343111395835876),\n",
       "    ('tomorow', 0.6145380735397339),\n",
       "    ('tmrw', 0.6117316484451294),\n",
       "    ('later', 0.5961143970489502),\n",
       "    ('2morrow', 0.58842933177948),\n",
       "    ('wednesday', 0.5760573744773865),\n",
       "    ('study', 0.5754880309104919),\n",
       "    ('thursday', 0.5693942308425903),\n",
       "    ('packing', 0.5689973831176758),\n",
       "    ('maths', 0.563879132270813)])]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading from file\n",
    "read_w2v = Word2Vec.load('word2vec.model')\n",
    "getMostSimilarWords(tweets, w2v_model=read_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(count_vectorizer, open('count_vectorizer.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}