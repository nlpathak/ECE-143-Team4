{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Stage + Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 12, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8f993c1d1aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# From https://www.kaggle.com/kazanova/sentiment140\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Added custom column header line to the csv after download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# df.columns = ['target','ids','date','flag','user','text']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 12, saw 2\n"
     ]
    }
   ],
   "source": [
    "# From https://www.kaggle.com/kazanova/sentiment140\n",
    "# Added custom column header line to the csv after download\n",
    "df = pd.read_csv('./data.csv')\n",
    "# df.columns = ['target','ids','date','flag','user','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "0             0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1             0  is upset that he can't update his Facebook by ...\n",
       "2             0  @Kenichan I dived many times for the ball. Man...\n",
       "3             0    my whole body feels itchy and like its on fire \n",
       "4             0  @nationwideclass no, it's not behaving at all....\n",
       "...         ...                                                ...\n",
       "1599995       4  Just woke up. Having no school is the best fee...\n",
       "1599996       4  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       4  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       4  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_df = df.drop(['ids', 'date', 'flag', 'user'], axis=1) # drop cols that aren't useful for our model\n",
    "final_large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Negative Sentiment Tweets: 800000\n",
      "Number of Neutral Sentiment Tweets: 0\n",
      "Number of Positive Sentiment Tweets: 800000\n"
     ]
    }
   ],
   "source": [
    "print('Number of Negative Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 0]))\n",
    "print('Number of Neutral Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 2]))\n",
    "print('Number of Positive Sentiment Tweets:', len(final_large_df[final_large_df['target'] == 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice no neutral sentiment data and way too much data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_large_df.target = final_large_df.target / 4 # convert the target column to 0 and 1 labels where 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "0           0.0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1           0.0  is upset that he can't update his Facebook by ...\n",
       "2           0.0  @Kenichan I dived many times for the ball. Man...\n",
       "3           0.0    my whole body feels itchy and like its on fire \n",
       "4           0.0  @nationwideclass no, it's not behaving at all....\n",
       "...         ...                                                ...\n",
       "1599995     1.0  Just woke up. Having no school is the best fee...\n",
       "1599996     1.0  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997     1.0  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998     1.0  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999     1.0  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_large_df.sample(800000)\n",
    "del final_large_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>God damn, I seriously think I busted an ear dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506770</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@Marama Actually we start this afternoon!  I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94816</th>\n",
       "      <td>0.0</td>\n",
       "      <td>he's back in portland  i'll see him on Sunday....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119309</th>\n",
       "      <td>1.0</td>\n",
       "      <td>can't wait to see the game tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562814</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@oceanUP would you like to be apart of the twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247454</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@CALiz3 can u plz pay my cell phone bill? It's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395033</th>\n",
       "      <td>0.0</td>\n",
       "      <td>bad news: http://tinyurl.com/lkcs56 (via @greg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078320</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@ashleycolucci i just got in and im debating t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998780</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@officialjule thx  I love yours, too &amp;lt;3 was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537460</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@paperweighted of course</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "203903      0.0  God damn, I seriously think I busted an ear dr...\n",
       "1506770     1.0  @Marama Actually we start this afternoon!  I w...\n",
       "94816       0.0  he's back in portland  i'll see him on Sunday....\n",
       "1119309     1.0               can't wait to see the game tonight. \n",
       "1562814     1.0  @oceanUP would you like to be apart of the twe...\n",
       "...         ...                                                ...\n",
       "247454      0.0  @CALiz3 can u plz pay my cell phone bill? It's...\n",
       "395033      0.0  bad news: http://tinyurl.com/lkcs56 (via @greg...\n",
       "1078320     1.0  @ashleycolucci i just got in and im debating t...\n",
       "998780      1.0  @officialjule thx  I love yours, too &lt;3 was...\n",
       "1537460     1.0                          @paperweighted of course \n",
       "\n",
       "[800000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Splits for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(final_df, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Stats:\n",
      "Size of Training Set: 640000\n",
      "Number of Negative Sentiment Tweets: 319302\n",
      "Number of Positive Sentiment Tweets: 320698\n"
     ]
    }
   ],
   "source": [
    "print('Training Set Stats:')\n",
    "print('Size of Training Set:', len(train_df))\n",
    "print('Number of Negative Sentiment Tweets:', len(train_df[train_df['target'] == 0]))\n",
    "print('Number of Positive Sentiment Tweets:', len(train_df[train_df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Stats:\n",
      "Size of Test Set: 160000\n",
      "Number of Negative Sentiment Tweets: 79746\n",
      "Number of Positive Sentiment Tweets: 80254\n"
     ]
    }
   ],
   "source": [
    "print('Test Set Stats:')\n",
    "print('Size of Test Set:', len(test_df))\n",
    "print('Number of Negative Sentiment Tweets:', len(test_df[test_df['target'] == 0]))\n",
    "print('Number of Positive Sentiment Tweets:', len(test_df[test_df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count-Vectorizer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=200000, tokenizer=tweetTokenizer.tokenize, ngram_range=(1,3))\n",
    "X_train = count_vectorizer.fit_transform([entry['text'] for i, entry in train_df.iterrows()])\n",
    "Y_train = np.array([int(entry['target']) for i, entry in train_df.iterrows()])\n",
    "\n",
    "X_test = count_vectorizer.transform([entry['text'] for i, entry in test_df.iterrows()])\n",
    "Y_test = np.array([int(entry['target']) for i, entry in test_df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=15000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_model = LogisticRegression(C = 0.1, max_iter=15000)\n",
    "count_vect_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.82131875\n",
      "Testing Accuracy: 0.78861875\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Accuracy: {np.mean(count_vect_model.predict(X_train) == Y_train)}')\n",
    "print(f'Testing Accuracy: {np.mean(count_vect_model.predict(X_test) == Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write everything to files\n",
    "pickle.dump(count_vectorizer, open('count_vectorizer.pickle', 'wb'))\n",
    "pickle.dump(count_vect_model, open('count_vect_model.pickle', 'wb'))\n",
    "scipy.sparse.save_npz('count_vect_X_train.npz', X_train)\n",
    "np.save('count_vect_Y_train.npy', Y_train)\n",
    "scipy.sparse.save_npz('count_vect_X_test.npz', X_test)\n",
    "np.save('count_vect_Y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled Training Accuracy: 0.82131875\n",
      "Pickled Testing Accuracy: 0.78861875\n"
     ]
    }
   ],
   "source": [
    "# Test that pickling is working\n",
    "read_vect = pickle.load(open('count_vectorizer.pickle', 'rb'))\n",
    "read_model = pickle.load(open('count_vect_model.pickle', 'rb'))\n",
    "\n",
    "read_X_train = scipy.sparse.load_npz('count_vect_X_train.npz')\n",
    "read_Y_train = np.load('count_vect_Y_train.npy')\n",
    "read_X_test = scipy.sparse.load_npz('count_vect_X_test.npz')\n",
    "read_Y_test = np.load('count_vect_Y_test.npy')\n",
    "print(f'Pickled Training Accuracy: {np.mean(read_model.predict(read_X_train) == read_Y_train)}')\n",
    "print(f'Pickled Testing Accuracy: {np.mean(read_model.predict(read_X_test) == read_Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82131875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check probabilities work as expected\n",
    "np.mean(np.argmax(read_model.predict_proba(read_X_train), axis=1) == read_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=200000, tokenizer=tweetTokenizer.tokenize, ngram_range=(1,3))\n",
    "X_train = tfidf_vectorizer.fit_transform([entry['text'] for i, entry in train_df.iterrows()])\n",
    "Y_train = np.array([int(entry['target']) for i, entry in train_df.iterrows()])\n",
    "\n",
    "X_test = tfidf_vectorizer.transform([entry['text'] for i, entry in test_df.iterrows()])\n",
    "Y_test = np.array([int(entry['target']) for i, entry in test_df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.5, max_iter=15000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model = LogisticRegression(C = 1.5, max_iter=15000)\n",
    "tfidf_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8407203125\n",
      "Testing Accuracy: 0.79053125\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Accuracy: {np.mean(tfidf_model.predict(X_train) == Y_train)}')\n",
    "print(f'Testing Accuracy: {np.mean(tfidf_model.predict(X_test) == Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vectorizer, open('tfidf_vect.pickle', 'wb'))\n",
    "pickle.dump(tfidf_model, open('tfidf_model.pickle', 'wb'))\n",
    "scipy.sparse.save_npz('tfidf_vect_X_train.npz', X_train)\n",
    "np.save('tfidf_vect_Y_train.npy', Y_train)\n",
    "scipy.sparse.save_npz('tfidf_vect_X_test.npz', X_test)\n",
    "np.save('tfidf_vect_Y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled Training Accuracy: 0.8407203125\n",
      "Pickled Testing Accuracy: 0.79053125\n"
     ]
    }
   ],
   "source": [
    "# Test that pickling is working\n",
    "read_vect = pickle.load(open('tfidf_vect.pickle', 'rb'))\n",
    "read_model = pickle.load(open('tfidf_model.pickle', 'rb'))\n",
    "\n",
    "read_X_train = scipy.sparse.load_npz('tfidf_vect_X_train.npz')\n",
    "read_Y_train = np.load('tfidf_vect_Y_train.npy')\n",
    "read_X_test = scipy.sparse.load_npz('tfidf_vect_X_test.npz')\n",
    "read_Y_test = np.load('tfidf_vect_Y_test.npy')\n",
    "print(f'Pickled Training Accuracy: {np.mean(read_model.predict(read_X_train) == read_Y_train)}')\n",
    "print(f'Pickled Testing Accuracy: {np.mean(read_model.predict(read_X_test) == read_Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Vectorizer Learned Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExtremeWords(vectorizer, model):\n",
    "    feature_names = np.array(read_vect.get_feature_names())\n",
    "    order = np.argsort(read_model.coef_)\n",
    "    \n",
    "    print(\"Top 50 Most Negative Words/Phrases in Order:\")\n",
    "    print(feature_names[order[0, :50]])\n",
    "    print()\n",
    "    print(\"Top 50 Most Positive Words/Phrases in Order:\")\n",
    "    print(feature_names[order[0, -50:]][::-1])\n",
    "    \n",
    "    return feature_names[order[0, :50]], feature_names[order[0, -50:]][::-1] # negative, positive    \n",
    "\n",
    "def predict(tweets, vectorizer, model):\n",
    "    tweet_vectors = vectorizer.transform(tweets)\n",
    "    preds = model.predict_proba(tweet_vectors)\n",
    "    returnList = []\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        print(f'Tweet: {tweet}')\n",
    "        pred = \"Negative\" if np.argmax(preds[i]) == 0 else \"Positive\"\n",
    "        print(f'Prediction: {pred}')\n",
    "        print(f'Confidence of {pred} Prediction (0 to 1): {np.max(preds[i])}')\n",
    "        print()\n",
    "        returnList.append((tweet, pred, np.max(preds[i])))\n",
    "    return returnList\n",
    "\n",
    "def analyzeTweets(tweets, vectorizer, model):\n",
    "    returnList = []\n",
    "    for tweet in tweets:\n",
    "        tweetList = []\n",
    "        for word in tweet.split():\n",
    "            word = word.lower()\n",
    "            if word in vectorizer.get_feature_names():\n",
    "                index = vectorizer.get_feature_names().index(word)\n",
    "                print(f'Word: {word}, Connotation: {model.coef_[0, index]:.3f}')\n",
    "                tweetList.append((word, model.coef_[0, index]))\n",
    "            else: # not a top feature\n",
    "                print(f'Word: {word}, Connotation: {0:.3f}')\n",
    "                tweetList.append((word, 0))\n",
    "        returnList.append(tweetList)\n",
    "        print()\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Most Negative Words/Phrases in Order:\n",
      "['sad' 'miss' 'sick' 'poor' 'missing' \"can't\" 'sadly' 'sucks' 'hurts'\n",
      " 'wish' 'headache' 'bummed' 'gutted' 'unfortunately' 'upset' 'ugh'\n",
      " 'disappointed' 'hate' 'lost' 'died' 'ruined' 'missed' 'rip' 'broke'\n",
      " 'bummer' 'depressing' 'disappointing' 'cancelled' 'lonely' 'depressed'\n",
      " 'misses' 'worst' 'broken' 'sore' \"didn't\" 'horrible' '. miss' 'sorry'\n",
      " 'missin' 'bad' 'hurt' 'crying' 'argh' 'shame' 'ouch' 'sigh' 'sux' 'hates'\n",
      " 'stuck' 'grrr']\n",
      "\n",
      "Top 50 Most Positive Words/Phrases in Order:\n",
      "[\"can't wait\" 'wish luck' 'thank' 'thanks' 'glad' 'welcome'\n",
      " 'congratulations' 'yay' '=(' 'proud' 'smile' 'awesome' '#followfriday'\n",
      " 'congrats' 'amazing' \"isn't bad\" \"don't forget\" 'great' 'hello' 'excited'\n",
      " 'smiling' 'yummy' 'hehe' 'thx' \"don't feel bad\" \"wasn't bad\" 'loving'\n",
      " 'pleasure' 'happy' 'feel free' 'blessed' 'hi' 'let know' 'hehehe' 'cute'\n",
      " 'wonderful' 'love' 'appreciate' 'hey' 'yey' 'enjoy' \"doesn't mean\"\n",
      " \"don't need\" 'sweet' \"don't worry\" 'yayyy' 'lovely' 'dont worry'\n",
      " 'bad ass' 'best']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['sad', 'miss', 'sick', 'poor', 'missing', \"can't\", 'sadly',\n",
       "        'sucks', 'hurts', 'wish', 'headache', 'bummed', 'gutted',\n",
       "        'unfortunately', 'upset', 'ugh', 'disappointed', 'hate', 'lost',\n",
       "        'died', 'ruined', 'missed', 'rip', 'broke', 'bummer', 'depressing',\n",
       "        'disappointing', 'cancelled', 'lonely', 'depressed', 'misses',\n",
       "        'worst', 'broken', 'sore', \"didn't\", 'horrible', '. miss', 'sorry',\n",
       "        'missin', 'bad', 'hurt', 'crying', 'argh', 'shame', 'ouch', 'sigh',\n",
       "        'sux', 'hates', 'stuck', 'grrr'], dtype='<U59'),\n",
       " array([\"can't wait\", 'wish luck', 'thank', 'thanks', 'glad', 'welcome',\n",
       "        'congratulations', 'yay', '=(', 'proud', 'smile', 'awesome',\n",
       "        '#followfriday', 'congrats', 'amazing', \"isn't bad\",\n",
       "        \"don't forget\", 'great', 'hello', 'excited', 'smiling', 'yummy',\n",
       "        'hehe', 'thx', \"don't feel bad\", \"wasn't bad\", 'loving',\n",
       "        'pleasure', 'happy', 'feel free', 'blessed', 'hi', 'let know',\n",
       "        'hehehe', 'cute', 'wonderful', 'love', 'appreciate', 'hey', 'yey',\n",
       "        'enjoy', \"doesn't mean\", \"don't need\", 'sweet', \"don't worry\",\n",
       "        'yayyy', 'lovely', 'dont worry', 'bad ass', 'best'], dtype='<U59'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read_vect = pickle.load(open('count_vectorizer.pickle', 'rb'))\n",
    "# read_model = pickle.load(open('count_vect_model.pickle', 'rb'))\n",
    "read_vect = pickle.load(open('tfidf_vect.pickle', 'rb'))\n",
    "read_model = pickle.load(open('tfidf_model.pickle', 'rb'))\n",
    "\n",
    "getExtremeWords(vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: This bag of chips is disgusting yuck\n",
      "Prediction: Negative\n",
      "Confidence of Negative Prediction (0 to 1): 0.9236253837917592\n",
      "\n",
      "Tweet: i really enjoy riding my bike\n",
      "Prediction: Positive\n",
      "Confidence of Positive Prediction (0 to 1): 0.7697452284057029\n",
      "\n",
      "Tweet: it will be 70 degrees tomorrow\n",
      "Prediction: Negative\n",
      "Confidence of Negative Prediction (0 to 1): 0.7083217410487461\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('This bag of chips is disgusting yuck', 'Negative', 0.9236253837917592),\n",
       " ('i really enjoy riding my bike', 'Positive', 0.7697452284057029),\n",
       " ('it will be 70 degrees tomorrow', 'Negative', 0.7083217410487461)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This bag of chips is disgusting yuck', 'i really enjoy riding my bike', 'it will be 70 degrees tomorrow']\n",
    "predict(tweets, vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: this, Connotation: 0.000\n",
      "Word: bag, Connotation: -0.062\n",
      "Word: of, Connotation: 0.000\n",
      "Word: chips, Connotation: -0.649\n",
      "Word: is, Connotation: 0.000\n",
      "Word: disgusting, Connotation: -2.302\n",
      "Word: yuck, Connotation: -3.207\n",
      "\n",
      "Word: i, Connotation: 0.000\n",
      "Word: really, Connotation: -2.087\n",
      "Word: enjoy, Connotation: 3.838\n",
      "Word: riding, Connotation: 0.813\n",
      "Word: my, Connotation: 0.000\n",
      "Word: bike, Connotation: -0.527\n",
      "\n",
      "Word: it, Connotation: 0.000\n",
      "Word: will, Connotation: 0.000\n",
      "Word: be, Connotation: 0.000\n",
      "Word: 70, Connotation: -0.873\n",
      "Word: degrees, Connotation: -1.040\n",
      "Word: tomorrow, Connotation: -0.977\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('this', 0),\n",
       "  ('bag', -0.06216028958684153),\n",
       "  ('of', 0),\n",
       "  ('chips', -0.6485009627456549),\n",
       "  ('is', 0),\n",
       "  ('disgusting', -2.302190173136221),\n",
       "  ('yuck', -3.206980065350292)],\n",
       " [('i', 0),\n",
       "  ('really', -2.087198473096131),\n",
       "  ('enjoy', 3.8379259600671456),\n",
       "  ('riding', 0.8131211297231977),\n",
       "  ('my', 0),\n",
       "  ('bike', -0.5266312079671122)],\n",
       " [('it', 0),\n",
       "  ('will', 0),\n",
       "  ('be', 0),\n",
       "  ('70', -0.8733351276175527),\n",
       "  ('degrees', -1.0402278742897086),\n",
       "  ('tomorrow', -0.9767936767873584)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzeTweets(tweets, vectorizer=read_vect, model=read_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model \n",
    "### Generate similarity scores of words for visualization\n",
    "Referenced https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/ and https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/nikhilpathak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [entry['text'].lower() for i, entry in train_df.iterrows()]\n",
    "words = [tweetTokenizer.tokenize(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(words, min_count=5)\n",
    "w2v.save('word2vec.model') # write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostSimilarWords(tweets, w2v_model):\n",
    "    returnList = []\n",
    "    for tweet in tweets:\n",
    "        tweetList = []\n",
    "        for word in tweetTokenizer.tokenize(tweet.lower()):\n",
    "            if word in w2v_model.wv.vocab:\n",
    "                tweetList.append((word, w2v_model.wv.most_similar(word)))\n",
    "            else:\n",
    "                tweetList.append((word, []))\n",
    "        returnList.append(tweetList)\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this',\n",
       "   [('that', 0.6022673845291138),\n",
       "    ('it', 0.5350900888442993),\n",
       "    ('next', 0.5192029476165771),\n",
       "    ('the', 0.5077482461929321),\n",
       "    ('which', 0.49560144543647766),\n",
       "    ('every', 0.46573951840400696),\n",
       "    ('a', 0.4238699674606323),\n",
       "    ('whole', 0.421683132648468),\n",
       "    ('another', 0.4039299488067627),\n",
       "    ('there', 0.3813204765319824)]),\n",
       "  ('bag',\n",
       "   [('pocket', 0.7603296637535095),\n",
       "    ('box', 0.7462670207023621),\n",
       "    ('shoe', 0.7241692543029785),\n",
       "    ('bowl', 0.7117549777030945),\n",
       "    ('bottle', 0.7101829648017883),\n",
       "    ('chair', 0.6970698237419128),\n",
       "    ('oil', 0.6969743967056274),\n",
       "    ('truck', 0.6936678290367126),\n",
       "    ('purse', 0.6935265064239502),\n",
       "    ('coat', 0.6912730932235718)]),\n",
       "  ('of',\n",
       "   [('recent', 0.4405097961425781),\n",
       "    (\"world's\", 0.42518895864486694),\n",
       "    ('tree', 0.42112934589385986),\n",
       "    ('stanley', 0.4121711850166321),\n",
       "    ('shed', 0.40368223190307617),\n",
       "    ('dozen', 0.39392614364624023),\n",
       "    ('canadian', 0.3781902492046356),\n",
       "    ('teams', 0.3653208315372467),\n",
       "    ('outer', 0.3647944927215576),\n",
       "    ('mainly', 0.36359184980392456)]),\n",
       "  ('chips',\n",
       "   [('rice', 0.8847311735153198),\n",
       "    ('strawberries', 0.884215235710144),\n",
       "    ('cheese', 0.8801683187484741),\n",
       "    ('bacon', 0.8731657266616821),\n",
       "    ('biscuits', 0.8697911500930786),\n",
       "    ('corn', 0.8686051368713379),\n",
       "    ('salad', 0.8635122179985046),\n",
       "    ('fries', 0.861212968826294),\n",
       "    ('fruit', 0.8587872982025146),\n",
       "    ('eggs', 0.8562004566192627)]),\n",
       "  ('is',\n",
       "   [(\"isn't\", 0.736416220664978),\n",
       "    ('isnt', 0.673029363155365),\n",
       "    ('keeps', 0.6019695997238159),\n",
       "    ('was', 0.5826245546340942),\n",
       "    ('gets', 0.5454555749893188),\n",
       "    (\"he's\", 0.5322301387786865),\n",
       "    (\"she's\", 0.5247272253036499),\n",
       "    ('hes', 0.5050098896026611),\n",
       "    ('works', 0.49816781282424927),\n",
       "    (\"wasn't\", 0.4958190321922302)]),\n",
       "  ('disgusting',\n",
       "   [('gross', 0.8098635077476501),\n",
       "    ('weird', 0.7527520656585693),\n",
       "    ('ridiculous', 0.7444571256637573),\n",
       "    ('awful', 0.7386634945869446),\n",
       "    ('painful', 0.7313137650489807),\n",
       "    ('ugly', 0.7184758186340332),\n",
       "    ('awkward', 0.7168139219284058),\n",
       "    ('unbelievable', 0.7161579728126526),\n",
       "    ('rubbish', 0.7093132734298706),\n",
       "    ('itchy', 0.7049988508224487)]),\n",
       "  ('yuck',\n",
       "   [('bleh', 0.6921120882034302),\n",
       "    ('ick', 0.6538533568382263),\n",
       "    ('urgh', 0.6520437002182007),\n",
       "    ('blah', 0.6490176916122437),\n",
       "    ('eww', 0.6330767869949341),\n",
       "    ('ewww', 0.6162062883377075),\n",
       "    ('grrrr', 0.6104581952095032),\n",
       "    ('blech', 0.6047781705856323),\n",
       "    ('eugh', 0.594567060470581),\n",
       "    ('ouch', 0.5937713980674744)])],\n",
       " [('i',\n",
       "   [('we', 0.6853910684585571),\n",
       "    ('you', 0.6344316005706787),\n",
       "    ('they', 0.6039923429489136),\n",
       "    ('u', 0.5776559114456177),\n",
       "    ('it', 0.5630919933319092),\n",
       "    ('them', 0.48748859763145447),\n",
       "    ('she', 0.48637497425079346),\n",
       "    ('he', 0.4719078540802002),\n",
       "    ('actually', 0.4627757668495178),\n",
       "    ('him', 0.462259978055954)]),\n",
       "  ('really',\n",
       "   [('kinda', 0.6788560152053833),\n",
       "    ('actually', 0.656516432762146),\n",
       "    ('so', 0.6079989671707153),\n",
       "    ('seriously', 0.587860643863678),\n",
       "    ('quite', 0.5774067640304565),\n",
       "    ('very', 0.5301282405853271),\n",
       "    ('too', 0.5226210355758667),\n",
       "    ('sooo', 0.5044684410095215),\n",
       "    ('sooooo', 0.5024033784866333),\n",
       "    ('soooo', 0.48644566535949707)]),\n",
       "  ('enjoy',\n",
       "   [('enjoying', 0.5976507663726807),\n",
       "    ('enjoyed', 0.574946403503418),\n",
       "    ('appreciate', 0.4785367250442505),\n",
       "    (\"how's\", 0.44359278678894043),\n",
       "    ('loving', 0.42369434237480164),\n",
       "    ('salvage', 0.42335593700408936),\n",
       "    ('spend', 0.41857486963272095),\n",
       "    ('njoy', 0.41310423612594604),\n",
       "    ('hows', 0.4130764603614807),\n",
       "    ('relax', 0.4112436771392822)]),\n",
       "  ('riding',\n",
       "   [('walking', 0.7123926877975464),\n",
       "    ('biking', 0.6802338361740112),\n",
       "    ('ride', 0.6780884265899658),\n",
       "    ('climbing', 0.6482816934585571),\n",
       "    ('rides', 0.6463833451271057),\n",
       "    ('rode', 0.634926438331604),\n",
       "    ('motorcycle', 0.6221483945846558),\n",
       "    ('lake', 0.6216177940368652),\n",
       "    ('river', 0.6186216473579407),\n",
       "    ('richmond', 0.6131294965744019)]),\n",
       "  ('my',\n",
       "   [('his', 0.7814475297927856),\n",
       "    ('her', 0.7365665435791016),\n",
       "    ('your', 0.6556292772293091),\n",
       "    ('their', 0.6017611026763916),\n",
       "    (\"someone's\", 0.5744596719741821),\n",
       "    ('our', 0.5620449185371399),\n",
       "    ('mah', 0.5507162809371948),\n",
       "    ('ma', 0.529866099357605),\n",
       "    ('mine', 0.49350202083587646),\n",
       "    ('ur', 0.4869818091392517)]),\n",
       "  ('bike',\n",
       "   [('bicycle', 0.7287511825561523),\n",
       "    ('motorcycle', 0.7116729021072388),\n",
       "    ('car', 0.6688809394836426),\n",
       "    ('lawn', 0.6579190492630005),\n",
       "    ('ride', 0.6450966596603394),\n",
       "    ('truck', 0.6424597501754761),\n",
       "    ('tire', 0.6274876594543457),\n",
       "    ('garage', 0.6196343898773193),\n",
       "    ('flat', 0.6112266778945923),\n",
       "    ('bikes', 0.5994158983230591)])],\n",
       " [('it',\n",
       "   [('that', 0.7850528955459595),\n",
       "    ('them', 0.6567940711975098),\n",
       "    ('everything', 0.6546316146850586),\n",
       "    ('mine', 0.6494866013526917),\n",
       "    ('he', 0.6227474212646484),\n",
       "    ('she', 0.585328221321106),\n",
       "    ('something', 0.5753048658370972),\n",
       "    ('anything', 0.5683326721191406),\n",
       "    ('i', 0.5630919933319092),\n",
       "    ('him', 0.5614441633224487)]),\n",
       "  ('will',\n",
       "   [(\"i'll\", 0.8257941007614136),\n",
       "    ('shall', 0.8251970410346985),\n",
       "    ('may', 0.8166158199310303),\n",
       "    (\"they'll\", 0.8095377683639526),\n",
       "    ('might', 0.8082070350646973),\n",
       "    (\"it'll\", 0.8080770373344421),\n",
       "    (\"she'll\", 0.79175865650177),\n",
       "    (\"we'll\", 0.7889803647994995),\n",
       "    ('should', 0.7870903015136719),\n",
       "    (\"he'll\", 0.7695443630218506)]),\n",
       "  ('be',\n",
       "   [('survive', 0.7073668241500854),\n",
       "    ('stay', 0.6914530992507935),\n",
       "    ('b', 0.6513534188270569),\n",
       "    ('die', 0.6467857360839844),\n",
       "    ('happen', 0.61439448595047),\n",
       "    ('continue', 0.6113724708557129),\n",
       "    ('start', 0.5967720150947571),\n",
       "    ('disappear', 0.5896309018135071),\n",
       "    ('retire', 0.5890316963195801),\n",
       "    ('consider', 0.5774808526039124)]),\n",
       "  ('70',\n",
       "   [('60', 0.8172014951705933),\n",
       "    ('80', 0.8094892501831055),\n",
       "    ('65', 0.8070707321166992),\n",
       "    ('35', 0.8046433925628662),\n",
       "    ('99', 0.795120358467102),\n",
       "    ('40', 0.7949656248092651),\n",
       "    ('90', 0.7939537167549133),\n",
       "    ('75', 0.7932224869728088),\n",
       "    ('50', 0.7832794785499573),\n",
       "    ('95', 0.7825549840927124)]),\n",
       "  ('degrees',\n",
       "   [('spf', 0.755501389503479),\n",
       "    ('degree', 0.7367409467697144),\n",
       "    ('celsius', 0.6631121635437012),\n",
       "    ('quid', 0.6454630494117737),\n",
       "    ('mph', 0.6401370167732239),\n",
       "    ('90', 0.6315370202064514),\n",
       "    ('%', 0.63138747215271),\n",
       "    ('cloudy', 0.6258422136306763),\n",
       "    ('deg', 0.6256710290908813),\n",
       "    ('bucks', 0.6246519088745117)]),\n",
       "  ('tomorrow',\n",
       "   [('tmrw', 0.7414162158966064),\n",
       "    ('tommorow', 0.7362726926803589),\n",
       "    ('tomorow', 0.7262307405471802),\n",
       "    ('tommorrow', 0.6744210124015808),\n",
       "    ('tmr', 0.6684221625328064),\n",
       "    ('2morrow', 0.6641130447387695),\n",
       "    ('2moro', 0.6270303130149841),\n",
       "    ('tonight', 0.617688775062561),\n",
       "    ('wednesday', 0.6139913201332092),\n",
       "    ('thursday', 0.6115055084228516)])]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This bag of chips is disgusting yuck', 'i really enjoy riding my bike', 'it will be 70 degrees tomorrow']\n",
    "getMostSimilarWords(tweets, w2v_model=w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this',\n",
       "   [('that', 0.6022673845291138),\n",
       "    ('it', 0.5350900888442993),\n",
       "    ('next', 0.5192029476165771),\n",
       "    ('the', 0.5077482461929321),\n",
       "    ('which', 0.49560144543647766),\n",
       "    ('every', 0.46573951840400696),\n",
       "    ('a', 0.4238699674606323),\n",
       "    ('whole', 0.421683132648468),\n",
       "    ('another', 0.4039299488067627),\n",
       "    ('there', 0.3813204765319824)]),\n",
       "  ('bag',\n",
       "   [('pocket', 0.7603296637535095),\n",
       "    ('box', 0.7462670207023621),\n",
       "    ('shoe', 0.7241692543029785),\n",
       "    ('bowl', 0.7117549777030945),\n",
       "    ('bottle', 0.7101829648017883),\n",
       "    ('chair', 0.6970698237419128),\n",
       "    ('oil', 0.6969743967056274),\n",
       "    ('truck', 0.6936678290367126),\n",
       "    ('purse', 0.6935265064239502),\n",
       "    ('coat', 0.6912730932235718)]),\n",
       "  ('of',\n",
       "   [('recent', 0.4405097961425781),\n",
       "    (\"world's\", 0.42518895864486694),\n",
       "    ('tree', 0.42112934589385986),\n",
       "    ('stanley', 0.4121711850166321),\n",
       "    ('shed', 0.40368223190307617),\n",
       "    ('dozen', 0.39392614364624023),\n",
       "    ('canadian', 0.3781902492046356),\n",
       "    ('teams', 0.3653208315372467),\n",
       "    ('outer', 0.3647944927215576),\n",
       "    ('mainly', 0.36359184980392456)]),\n",
       "  ('chips',\n",
       "   [('rice', 0.8847311735153198),\n",
       "    ('strawberries', 0.884215235710144),\n",
       "    ('cheese', 0.8801683187484741),\n",
       "    ('bacon', 0.8731657266616821),\n",
       "    ('biscuits', 0.8697911500930786),\n",
       "    ('corn', 0.8686051368713379),\n",
       "    ('salad', 0.8635122179985046),\n",
       "    ('fries', 0.861212968826294),\n",
       "    ('fruit', 0.8587872982025146),\n",
       "    ('eggs', 0.8562004566192627)]),\n",
       "  ('is',\n",
       "   [(\"isn't\", 0.736416220664978),\n",
       "    ('isnt', 0.673029363155365),\n",
       "    ('keeps', 0.6019695997238159),\n",
       "    ('was', 0.5826245546340942),\n",
       "    ('gets', 0.5454555749893188),\n",
       "    (\"he's\", 0.5322301387786865),\n",
       "    (\"she's\", 0.5247272253036499),\n",
       "    ('hes', 0.5050098896026611),\n",
       "    ('works', 0.49816781282424927),\n",
       "    (\"wasn't\", 0.4958190321922302)]),\n",
       "  ('disgusting',\n",
       "   [('gross', 0.8098635077476501),\n",
       "    ('weird', 0.7527520656585693),\n",
       "    ('ridiculous', 0.7444571256637573),\n",
       "    ('awful', 0.7386634945869446),\n",
       "    ('painful', 0.7313137650489807),\n",
       "    ('ugly', 0.7184758186340332),\n",
       "    ('awkward', 0.7168139219284058),\n",
       "    ('unbelievable', 0.7161579728126526),\n",
       "    ('rubbish', 0.7093132734298706),\n",
       "    ('itchy', 0.7049988508224487)]),\n",
       "  ('yuck',\n",
       "   [('bleh', 0.6921120882034302),\n",
       "    ('ick', 0.6538533568382263),\n",
       "    ('urgh', 0.6520437002182007),\n",
       "    ('blah', 0.6490176916122437),\n",
       "    ('eww', 0.6330767869949341),\n",
       "    ('ewww', 0.6162062883377075),\n",
       "    ('grrrr', 0.6104581952095032),\n",
       "    ('blech', 0.6047781705856323),\n",
       "    ('eugh', 0.594567060470581),\n",
       "    ('ouch', 0.5937713980674744)])],\n",
       " [('i',\n",
       "   [('we', 0.6853910684585571),\n",
       "    ('you', 0.6344316005706787),\n",
       "    ('they', 0.6039923429489136),\n",
       "    ('u', 0.5776559114456177),\n",
       "    ('it', 0.5630919933319092),\n",
       "    ('them', 0.48748859763145447),\n",
       "    ('she', 0.48637497425079346),\n",
       "    ('he', 0.4719078540802002),\n",
       "    ('actually', 0.4627757668495178),\n",
       "    ('him', 0.462259978055954)]),\n",
       "  ('really',\n",
       "   [('kinda', 0.6788560152053833),\n",
       "    ('actually', 0.656516432762146),\n",
       "    ('so', 0.6079989671707153),\n",
       "    ('seriously', 0.587860643863678),\n",
       "    ('quite', 0.5774067640304565),\n",
       "    ('very', 0.5301282405853271),\n",
       "    ('too', 0.5226210355758667),\n",
       "    ('sooo', 0.5044684410095215),\n",
       "    ('sooooo', 0.5024033784866333),\n",
       "    ('soooo', 0.48644566535949707)]),\n",
       "  ('enjoy',\n",
       "   [('enjoying', 0.5976507663726807),\n",
       "    ('enjoyed', 0.574946403503418),\n",
       "    ('appreciate', 0.4785367250442505),\n",
       "    (\"how's\", 0.44359278678894043),\n",
       "    ('loving', 0.42369434237480164),\n",
       "    ('salvage', 0.42335593700408936),\n",
       "    ('spend', 0.41857486963272095),\n",
       "    ('njoy', 0.41310423612594604),\n",
       "    ('hows', 0.4130764603614807),\n",
       "    ('relax', 0.4112436771392822)]),\n",
       "  ('riding',\n",
       "   [('walking', 0.7123926877975464),\n",
       "    ('biking', 0.6802338361740112),\n",
       "    ('ride', 0.6780884265899658),\n",
       "    ('climbing', 0.6482816934585571),\n",
       "    ('rides', 0.6463833451271057),\n",
       "    ('rode', 0.634926438331604),\n",
       "    ('motorcycle', 0.6221483945846558),\n",
       "    ('lake', 0.6216177940368652),\n",
       "    ('river', 0.6186216473579407),\n",
       "    ('richmond', 0.6131294965744019)]),\n",
       "  ('my',\n",
       "   [('his', 0.7814475297927856),\n",
       "    ('her', 0.7365665435791016),\n",
       "    ('your', 0.6556292772293091),\n",
       "    ('their', 0.6017611026763916),\n",
       "    (\"someone's\", 0.5744596719741821),\n",
       "    ('our', 0.5620449185371399),\n",
       "    ('mah', 0.5507162809371948),\n",
       "    ('ma', 0.529866099357605),\n",
       "    ('mine', 0.49350202083587646),\n",
       "    ('ur', 0.4869818091392517)]),\n",
       "  ('bike',\n",
       "   [('bicycle', 0.7287511825561523),\n",
       "    ('motorcycle', 0.7116729021072388),\n",
       "    ('car', 0.6688809394836426),\n",
       "    ('lawn', 0.6579190492630005),\n",
       "    ('ride', 0.6450966596603394),\n",
       "    ('truck', 0.6424597501754761),\n",
       "    ('tire', 0.6274876594543457),\n",
       "    ('garage', 0.6196343898773193),\n",
       "    ('flat', 0.6112266778945923),\n",
       "    ('bikes', 0.5994158983230591)])],\n",
       " [('it',\n",
       "   [('that', 0.7850528955459595),\n",
       "    ('them', 0.6567940711975098),\n",
       "    ('everything', 0.6546316146850586),\n",
       "    ('mine', 0.6494866013526917),\n",
       "    ('he', 0.6227474212646484),\n",
       "    ('she', 0.585328221321106),\n",
       "    ('something', 0.5753048658370972),\n",
       "    ('anything', 0.5683326721191406),\n",
       "    ('i', 0.5630919933319092),\n",
       "    ('him', 0.5614441633224487)]),\n",
       "  ('will',\n",
       "   [(\"i'll\", 0.8257941007614136),\n",
       "    ('shall', 0.8251970410346985),\n",
       "    ('may', 0.8166158199310303),\n",
       "    (\"they'll\", 0.8095377683639526),\n",
       "    ('might', 0.8082070350646973),\n",
       "    (\"it'll\", 0.8080770373344421),\n",
       "    (\"she'll\", 0.79175865650177),\n",
       "    (\"we'll\", 0.7889803647994995),\n",
       "    ('should', 0.7870903015136719),\n",
       "    (\"he'll\", 0.7695443630218506)]),\n",
       "  ('be',\n",
       "   [('survive', 0.7073668241500854),\n",
       "    ('stay', 0.6914530992507935),\n",
       "    ('b', 0.6513534188270569),\n",
       "    ('die', 0.6467857360839844),\n",
       "    ('happen', 0.61439448595047),\n",
       "    ('continue', 0.6113724708557129),\n",
       "    ('start', 0.5967720150947571),\n",
       "    ('disappear', 0.5896309018135071),\n",
       "    ('retire', 0.5890316963195801),\n",
       "    ('consider', 0.5774808526039124)]),\n",
       "  ('70',\n",
       "   [('60', 0.8172014951705933),\n",
       "    ('80', 0.8094892501831055),\n",
       "    ('65', 0.8070707321166992),\n",
       "    ('35', 0.8046433925628662),\n",
       "    ('99', 0.795120358467102),\n",
       "    ('40', 0.7949656248092651),\n",
       "    ('90', 0.7939537167549133),\n",
       "    ('75', 0.7932224869728088),\n",
       "    ('50', 0.7832794785499573),\n",
       "    ('95', 0.7825549840927124)]),\n",
       "  ('degrees',\n",
       "   [('spf', 0.755501389503479),\n",
       "    ('degree', 0.7367409467697144),\n",
       "    ('celsius', 0.6631121635437012),\n",
       "    ('quid', 0.6454630494117737),\n",
       "    ('mph', 0.6401370167732239),\n",
       "    ('90', 0.6315370202064514),\n",
       "    ('%', 0.63138747215271),\n",
       "    ('cloudy', 0.6258422136306763),\n",
       "    ('deg', 0.6256710290908813),\n",
       "    ('bucks', 0.6246519088745117)]),\n",
       "  ('tomorrow',\n",
       "   [('tmrw', 0.7414162158966064),\n",
       "    ('tommorow', 0.7362726926803589),\n",
       "    ('tomorow', 0.7262307405471802),\n",
       "    ('tommorrow', 0.6744210124015808),\n",
       "    ('tmr', 0.6684221625328064),\n",
       "    ('2morrow', 0.6641130447387695),\n",
       "    ('2moro', 0.6270303130149841),\n",
       "    ('tonight', 0.617688775062561),\n",
       "    ('wednesday', 0.6139913201332092),\n",
       "    ('thursday', 0.6115055084228516)])]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading from file\n",
    "read_w2v = Word2Vec.load('word2vec.model')\n",
    "getMostSimilarWords(tweets, w2v_model=read_w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
